{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "# read data\n",
    "movies = pd.read_csv(\"data/ml-20m/movies.csv\")\n",
    "ratings = pd.read_csv(\"data/ml-20m/ratings.csv\")\n",
    "\n",
    "# join\n",
    "ratings_joined = pd.merge(ratings, movies)\n",
    "\n",
    "# ratingsをsparse matrixに変換して横持ちにする\n",
    "action_adventure_ratings = ratings_joined.query(\"genres.str.contains('Action') or genres.str.contains('Adventure')\", \n",
    "                                                engine='python').reset_index(drop=True)\n",
    "# indexing ids\n",
    "# userid\n",
    "userid_unique = pd.Series(action_adventure_ratings[\"userId\"].unique())\n",
    "index_userid_dict = userid_unique.to_dict()\n",
    "# inverse\n",
    "userid_index_dict = dict(map(reversed, index_userid_dict.items()))\n",
    "\n",
    "# itemid\n",
    "itemid_unique = pd.Series(action_adventure_ratings[\"movieId\"].unique())\n",
    "index_itemid_dict = itemid_unique.to_dict()\n",
    "# inverse\n",
    "itemid_index_dict = dict(map(reversed, index_itemid_dict.items()))\n",
    "\n",
    "action_adventure_ratings[\"user_id\"] = action_adventure_ratings[\"userId\"].map(userid_index_dict)\n",
    "action_adventure_ratings[\"item_id\"] = action_adventure_ratings[\"movieId\"].map(itemid_index_dict)\n",
    "\n",
    "# reindexしたidを使って、アイテムとジャンルの対応が取れるdictを作る\n",
    "itemid_genres_dict = action_adventure_ratings[['item_id', 'genres']].set_index('item_id')['genres'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "X_train = cloudpickle.load(open(\"output/X_train.pkl\",\"rb\"))\n",
    "X_test = cloudpickle.load(open(\"output/X_test.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregateのtrainをactionとadventureに分離する\n",
    "# actionの列\n",
    "action_columns = [v for v in range(X_train.shape[1]) if 'Action' in itemid_genres_dict[v]]\n",
    "# adventureの列\n",
    "adventure_columns = [v for v in range(X_train.shape[1]) if 'Adventure' in itemid_genres_dict[v]]\n",
    "\n",
    "# 選んだカラムに応じてとってくる\n",
    "action_train = X_train[:, action_columns]\n",
    "adventure_train = X_train[:, adventure_columns]\n",
    "\n",
    "# adventureのみ、アイテムidのconcatとの対応関係が必要なので辞書として持っておく\n",
    "adventure_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'Adventure' in itemid_genres_dict[v]:\n",
    "        adventure_concat_itemid_dict[v] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アイテムidのconcatとの対応関係が必要なので辞書として持っておく\n",
    "action_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'Action' in itemid_genres_dict[v]:\n",
    "        action_concat_itemid_dict[v] = count\n",
    "        count += 1\n",
    "# inverse\n",
    "inverse_action_concat_itemid_dict = dict(map(reversed, action_concat_itemid_dict.items()))\n",
    "\n",
    "adventure_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'Adventure' in itemid_genres_dict[v]:\n",
    "        adventure_concat_itemid_dict[v] = count\n",
    "        count += 1\n",
    "# inverse\n",
    "inverse_adventure_concat_itemid_dict = dict(map(reversed, adventure_concat_itemid_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれにアクションしていないユーザを削る\n",
    "# 全ユーザと、削ったあとでの対応関係を辞書として持っておく\n",
    "action_train_selected = action_train[action_train.getnnz(1)>0]\n",
    "adventure_train_selected = adventure_train[adventure_train.getnnz(1)>0]\n",
    "\n",
    "action_train_action_users = {}\n",
    "action_users = action_train.getnnz(1)>0\n",
    "count = 0\n",
    "for i in range(action_train.shape[0]):\n",
    "    if action_users[i]:\n",
    "        action_train_action_users[i] = count\n",
    "        count += 1\n",
    "\n",
    "# inverse\n",
    "inverse_action_train_action_users = dict(map(reversed, action_train_action_users.items()))\n",
    "\n",
    "adventure_train_action_users = {}\n",
    "adventure_users = adventure_train.getnnz(1)>0\n",
    "count = 0\n",
    "for i in range(adventure_train.shape[0]):\n",
    "    if adventure_users[i]:\n",
    "        adventure_train_action_users[i] = count\n",
    "        count += 1\n",
    "\n",
    "# inverse\n",
    "inverse_adventure_train_action_users = dict(map(reversed, adventure_train_action_users.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれでNMFする\n",
    "# 今回は mediateでやったときのものを使う\n",
    "action_NMF = cloudpickle.load(open(\"output/action_NMF.pkl\",\"rb\"))\n",
    "adventure_NMF = cloudpickle.load(open(\"output/adventure_NMF.pkl\",\"rb\"))\n",
    "\n",
    "action_NMF_user_vectors = action_NMF.transform(action_train_selected)\n",
    "adventure_NMF_user_vectors = adventure_NMF.transform(adventure_train_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138389/138389 [00:02<00:00, 51242.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# actionとadventureでoverlapしているユーザで、ベクトルの対応表を作る\n",
    "overlap_action_user_vectors = []\n",
    "overlap_adventure_user_vectors = []\n",
    "count = 0\n",
    "for u in tqdm(range(X_train.shape[0])):\n",
    "    if u in action_train_action_users and u in adventure_train_action_users:\n",
    "        overlap_action_user_vectors.append(action_NMF_user_vectors[action_train_action_users[u]].tolist())\n",
    "        overlap_adventure_user_vectors.append(adventure_NMF_user_vectors[adventure_train_action_users[u]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoderの学習をする\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def build_model(input_dim, output_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation='relu')(inputs)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(output_dim, activation='sigmoid')(decoded)\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_vector, X_test_vector, y_train_vector, y_test_vector = train_test_split(overlap_action_user_vectors, overlap_adventure_user_vectors, random_state=42)\n",
    "X_train_vector, X_val_vector, y_train_vector, y_val_vector = train_test_split(X_train_vector, y_train_vector, random_state=42)\n",
    "\n",
    "epoch_size = 100\n",
    "batch_size = 256\n",
    "mcheck = ModelCheckpoint(\n",
    "    'output/ml-20m-model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "es_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "model = build_model(np.array(X_train_vector).shape[1], np.array(y_train_vector).shape[1])\n",
    "model.fit(\n",
    "    np.array(X_train_vector),\n",
    "    np.array(y_train_vector),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_size,\n",
    "    validation_data=(\n",
    "        np.array(X_val_vector),\n",
    "        np.array(y_val_vector)),\n",
    "    callbacks=[\n",
    "        mcheck,\n",
    "        es_cb],\n",
    "    shuffle=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.019493561228110558\n"
     ]
    }
   ],
   "source": [
    "# テストデータに対するRMSE計算\n",
    "from sklearn.metrics import mean_squared_error\n",
    "best_model = load_model('output/ml-20m-model.h5')\n",
    "y_pred = best_model.predict(np.array(X_test_vector))\n",
    "rmse_ = np.sqrt(mean_squared_error(y_pred, np.array(y_test_vector)))\n",
    "print('rmse: {}'.format(rmse_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaeとで、rmseを比較し、良い方を選ぶ\n",
    "# ref. https://keras.io/examples/variational_autoencoder/\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "class VAE():\n",
    "    def __init__(self, input_dim, intermediate_dim, latent_dim, original_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.original_dim = original_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.z_mean = None\n",
    "        self.z_log_var = None\n",
    "\n",
    "\n",
    "    # reparameterization trick\n",
    "    # instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "    # z = z_mean + sqrt(var) * epsilon\n",
    "    def sampling(self, args):\n",
    "        \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "        # Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "        # Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean = 0 and std = 1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "    def vae_loss(self, y_true, y_pred):\n",
    "        reconstruction_loss = mse(y_true, y_pred)\n",
    "        reconstruction_loss *= self.original_dim\n",
    "        kl_loss = 1 + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        return vae_loss\n",
    "\n",
    "\n",
    "    def build_vae(self):\n",
    "        # VAE model = encoder + decoder\n",
    "        # build encoder model\n",
    "        inputs = Input(shape=(self.input_dim,), name='encoder_input')\n",
    "        x = Dense(self.intermediate_dim, activation='relu')(inputs)\n",
    "        self.z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        self.z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "\n",
    "        # use reparameterization trick to push the sampling out as input\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(self.sampling, output_shape=(self.latent_dim,), name='z')([self.z_mean, self.z_log_var])\n",
    "\n",
    "        # instantiate encoder model\n",
    "        encoder = Model(inputs, [self.z_mean, self.z_log_var, z], name='encoder')\n",
    "\n",
    "        # build decoder model\n",
    "        latent_inputs = Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        x = Dense(self.intermediate_dim, activation='relu')(latent_inputs)\n",
    "        outputs = Dense(self.original_dim, activation='sigmoid')(x)\n",
    "\n",
    "        # instantiate decoder model\n",
    "        decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "        # instantiate VAE model\n",
    "        outputs = decoder(encoder(inputs)[2])\n",
    "        vae = Model(inputs, outputs, name='vae')\n",
    "        vae.compile(optimizer='adam', loss=self.vae_loss)\n",
    "        return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(np.array(X_train).shape[1], 256, 2, np.array(y_train).shape[1])\n",
    "model = vae.build_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76821 samples, validate on 25608 samples\n",
      "Epoch 1/100\n",
      "76821/76821 [==============================] - 2s 26us/step - loss: 2.8106 - val_loss: 0.1553\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15526, saving model to output/ml-20m-vae.h5\n",
      "Epoch 2/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1341 - val_loss: 0.1282\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15526 to 0.12820, saving model to output/ml-20m-vae.h5\n",
      "Epoch 3/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1247 - val_loss: 0.1248\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12820 to 0.12479, saving model to output/ml-20m-vae.h5\n",
      "Epoch 4/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1229 - val_loss: 0.1241\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12479 to 0.12410, saving model to output/ml-20m-vae.h5\n",
      "Epoch 5/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1223 - val_loss: 0.1237\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12410 to 0.12371, saving model to output/ml-20m-vae.h5\n",
      "Epoch 6/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1219 - val_loss: 0.1234\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12371 to 0.12345, saving model to output/ml-20m-vae.h5\n",
      "Epoch 7/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1213 - val_loss: 0.1229\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12345 to 0.12293, saving model to output/ml-20m-vae.h5\n",
      "Epoch 8/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1208 - val_loss: 0.1222\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12293 to 0.12216, saving model to output/ml-20m-vae.h5\n",
      "Epoch 9/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1203 - val_loss: 0.1219\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12216 to 0.12189, saving model to output/ml-20m-vae.h5\n",
      "Epoch 10/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1200 - val_loss: 0.1212\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12189 to 0.12123, saving model to output/ml-20m-vae.h5\n",
      "Epoch 11/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1196 - val_loss: 0.1211\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12123 to 0.12111, saving model to output/ml-20m-vae.h5\n",
      "Epoch 12/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1192 - val_loss: 0.1206\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.12111 to 0.12057, saving model to output/ml-20m-vae.h5\n",
      "Epoch 13/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1188 - val_loss: 0.1202\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.12057 to 0.12018, saving model to output/ml-20m-vae.h5\n",
      "Epoch 14/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1184 - val_loss: 0.1199\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.12018 to 0.11993, saving model to output/ml-20m-vae.h5\n",
      "Epoch 15/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1181 - val_loss: 0.1197\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11993 to 0.11971, saving model to output/ml-20m-vae.h5\n",
      "Epoch 16/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1180 - val_loss: 0.1195\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11971 to 0.11949, saving model to output/ml-20m-vae.h5\n",
      "Epoch 17/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1177 - val_loss: 0.1192\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11949 to 0.11920, saving model to output/ml-20m-vae.h5\n",
      "Epoch 18/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1176 - val_loss: 0.1191\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.11920 to 0.11915, saving model to output/ml-20m-vae.h5\n",
      "Epoch 19/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1175 - val_loss: 0.1190\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11915 to 0.11902, saving model to output/ml-20m-vae.h5\n",
      "Epoch 20/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1174 - val_loss: 0.1190\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.11902 to 0.11896, saving model to output/ml-20m-vae.h5\n",
      "Epoch 21/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1174 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.11896 to 0.11892, saving model to output/ml-20m-vae.h5\n",
      "Epoch 22/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1173 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.11892 to 0.11890, saving model to output/ml-20m-vae.h5\n",
      "Epoch 23/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1173 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.11890 to 0.11885, saving model to output/ml-20m-vae.h5\n",
      "Epoch 24/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1173 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.11885\n",
      "Epoch 25/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1188\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.11885 to 0.11882, saving model to output/ml-20m-vae.h5\n",
      "Epoch 26/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1172 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.11882\n",
      "Epoch 27/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.11882\n",
      "Epoch 28/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11882\n",
      "Epoch 29/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1173 - val_loss: 0.1188\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11882\n",
      "Epoch 30/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11882\n",
      "Epoch 31/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1188\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11882\n",
      "Epoch 32/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1188\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.11882 to 0.11880, saving model to output/ml-20m-vae.h5\n",
      "Epoch 33/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1188\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.11880 to 0.11880, saving model to output/ml-20m-vae.h5\n",
      "Epoch 34/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1190\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11880\n",
      "Epoch 35/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1188\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11880\n",
      "Epoch 36/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1173 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11880\n",
      "Epoch 37/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1173 - val_loss: 0.1188\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11880\n",
      "Epoch 38/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11880\n",
      "Epoch 39/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1172 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11880\n",
      "Epoch 40/100\n",
      "76821/76821 [==============================] - 1s 17us/step - loss: 0.1173 - val_loss: 0.1188\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11880\n",
      "Epoch 41/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1173 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11880\n",
      "Epoch 42/100\n",
      "76821/76821 [==============================] - 1s 19us/step - loss: 0.1173 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11880\n",
      "Epoch 43/100\n",
      "76821/76821 [==============================] - 1s 18us/step - loss: 0.1173 - val_loss: 0.1188\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11880\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc8009a16a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size = 100\n",
    "batch_size = 256\n",
    "mcheck = ModelCheckpoint(\n",
    "    'output/ml-20m-vae.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "es_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "model.fit(\n",
    "    np.array(X_train_vector),\n",
    "    np.array(y_train_vector),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_size,\n",
    "    validation_data=(\n",
    "        np.array(X_val_vector),\n",
    "        np.array(y_val_vector)),\n",
    "    callbacks=[\n",
    "        mcheck,\n",
    "        es_cb],\n",
    "    shuffle=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.03430834535706416\n"
     ]
    }
   ],
   "source": [
    "# テストデータに対するRMSE計算\n",
    "from sklearn.metrics import mean_squared_error\n",
    "best_model_vae = vae.build_vae()\n",
    "best_model_vae.load_weights('output/ml-20m-vae.h5')\n",
    "y_pred = best_model_vae.predict(np.array(X_test_vector))\n",
    "rmse_ = np.sqrt(mean_squared_error(y_pred, np.array(y_test_vector)))\n",
    "print('rmse: {}'.format(rmse_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(VAEのやりかたが悪かったようなだけな気もするが)今回は素のautoencoderを採用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138389/138389 [00:28<00:00, 4845.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# 評価対象のユーザ\n",
    "test_adventure_pos_items_dict = {}\n",
    "for i in tqdm(range(X_test.shape[0])):\n",
    "    # trainでadventureにアクションしていないユーザに\n",
    "    rated_items = X_train[i, :].indices\n",
    "    if len([v for v in rated_items if 'Adventure' in itemid_genres_dict[v]]) == 0:\n",
    "        # X_testの中でstoreしているアイテムが0以上のユーザに\n",
    "        if X_test[i, :].nnz > 0:\n",
    "            test_items = []\n",
    "            selected_user_ratings = X_test[i, :]\n",
    "            value_indices = selected_user_ratings.indices\n",
    "            sorted_indices = np.argsort(-X_test[i, :].toarray())[0]\n",
    "            # valueがあるアイテムのジャンルがadventureの場合に\n",
    "            for v in sorted_indices[:len(value_indices)]:\n",
    "                if 'Adventure' in itemid_genres_dict[v]:\n",
    "                    test_items.append(v)\n",
    "            if len(test_items) > 0:\n",
    "                test_adventure_pos_items_dict[i] = test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adventureのitemのベクトル\n",
    "adventure_item_vectors = adventure_NMF.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_user_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 643/643 [00:01<00:00, 437.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.recommend_util import ndcg\n",
    "ndcgs = {\n",
    "    'ndcg5':  [],\n",
    "    'ndcg10':  [],\n",
    "    'ndcg20':  [],\n",
    "    'ndcg50':  [],\n",
    "    'ndcg100':  []\n",
    "}\n",
    "count = 0\n",
    "best_model = load_model('output/ml-20m-model.h5')\n",
    "\n",
    "for userid, pos_items in tqdm(test_adventure_pos_items_dict.items()):\n",
    "   # pos_itemsをadventure_matrixの次元に変換する\n",
    "    pos_items = np.array([adventure_concat_itemid_dict[v] for v in pos_items])\n",
    "    # useridに対応するユーザベクトル(action)を得る\n",
    "    try:\n",
    "        action_userid = action_train_action_users[userid]\n",
    "    except:\n",
    "        count += 1\n",
    "        # 推薦できないユーザの場合は無条件で0を入れる\n",
    "        ndcgs['ndcg5'].append(0)\n",
    "        ndcgs['ndcg10'].append(0)\n",
    "        ndcgs['ndcg20'].append(0)\n",
    "        ndcgs['ndcg50'].append(0)\n",
    "        ndcgs['ndcg100'].append(0)\n",
    "        continue\n",
    "        \n",
    "    action_user_vector = action_NMF_user_vectors[action_userid, :]\n",
    "    # autoencoderを使ってadventureの次元に変換する\n",
    "    adventure_user_vector_action_AE = best_model.predict(action_user_vector.reshape(1, -1))\n",
    "    # adventureのitemのベクトルと掛け合わせる\n",
    "    adv_predict = np.dot(adventure_user_vector_action_AE, adventure_item_vectors)\n",
    "    # sum_ratingsをargsort\n",
    "    sorted_indices = np.array([v for v in np.argsort(-adv_predict)])[0]\n",
    "    ndcgs['ndcg5'].append(ndcg(sorted_indices[:5], pos_items))\n",
    "    ndcgs['ndcg10'].append(ndcg(sorted_indices[:10], pos_items))\n",
    "    ndcgs['ndcg20'].append(ndcg(sorted_indices[:20], pos_items))\n",
    "    ndcgs['ndcg50'].append(ndcg(sorted_indices[:50], pos_items))\n",
    "    ndcgs['ndcg100'].append(ndcg(sorted_indices[:100], pos_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@5: 0.041694927549288024\n",
      "ndcg@10: 0.05621393671488131\n",
      "ndcg@20: 0.06599918424990463\n",
      "ndcg@50: 0.08285782158282089\n",
      "ndcg@100: 0.09670554592765138\n"
     ]
    }
   ],
   "source": [
    "print(\"ndcg@5: {}\".format(np.mean(ndcgs['ndcg5'])))\n",
    "print(\"ndcg@10: {}\".format(np.mean(ndcgs['ndcg10'])))\n",
    "print(\"ndcg@20: {}\".format(np.mean(ndcgs['ndcg20'])))\n",
    "print(\"ndcg@50: {}\".format(np.mean(ndcgs['ndcg50'])))\n",
    "print(\"ndcg@100: {}\".format(np.mean(ndcgs['ndcg100'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
