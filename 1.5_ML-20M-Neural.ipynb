{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "# read data\n",
    "movies = pd.read_csv(\"data/ml-20m/movies.csv\")\n",
    "ratings = pd.read_csv(\"data/ml-20m/ratings.csv\")\n",
    "\n",
    "# join\n",
    "ratings_joined = pd.merge(ratings, movies)\n",
    "\n",
    "# ratingsをsparse matrixに変換して横持ちにする\n",
    "action_adventure_ratings = ratings_joined.query(\"genres.str.contains('Action') or genres.str.contains('Adventure')\", \n",
    "                                                engine='python').reset_index(drop=True)\n",
    "# indexing ids\n",
    "# userid\n",
    "userid_unique = pd.Series(action_adventure_ratings[\"userId\"].unique())\n",
    "index_userid_dict = userid_unique.to_dict()\n",
    "# inverse\n",
    "userid_index_dict = dict(map(reversed, index_userid_dict.items()))\n",
    "\n",
    "# itemid\n",
    "itemid_unique = pd.Series(action_adventure_ratings[\"movieId\"].unique())\n",
    "index_itemid_dict = itemid_unique.to_dict()\n",
    "# inverse\n",
    "itemid_index_dict = dict(map(reversed, index_itemid_dict.items()))\n",
    "\n",
    "action_adventure_ratings[\"user_id\"] = action_adventure_ratings[\"userId\"].map(userid_index_dict)\n",
    "action_adventure_ratings[\"item_id\"] = action_adventure_ratings[\"movieId\"].map(itemid_index_dict)\n",
    "\n",
    "# reindexしたidを使って、アイテムとジャンルの対応が取れるdictを作る\n",
    "itemid_genres_dict = action_adventure_ratings[['item_id', 'genres']].set_index('item_id')['genres'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "X_train = cloudpickle.load(open(\"output/ML-20M-X_train.pkl\",\"rb\"))\n",
    "X_test = cloudpickle.load(open(\"output/ML-20M-X_test.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregateのtrainをactionとadventureに分離する\n",
    "# actionの列\n",
    "action_columns = [v for v in range(X_train.shape[1]) if 'Action' in itemid_genres_dict[v]]\n",
    "# adventureの列\n",
    "adventure_columns = [v for v in range(X_train.shape[1]) if 'Adventure' in itemid_genres_dict[v]]\n",
    "\n",
    "# 選んだカラムに応じてとってくる\n",
    "action_train = X_train[:, action_columns]\n",
    "adventure_train = X_train[:, adventure_columns]\n",
    "\n",
    "# adventureのみ、アイテムidのconcatとの対応関係が必要なので辞書として持っておく\n",
    "adventure_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'Adventure' in itemid_genres_dict[v]:\n",
    "        adventure_concat_itemid_dict[v] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アイテムidのconcatとの対応関係が必要なので辞書として持っておく\n",
    "action_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'Action' in itemid_genres_dict[v]:\n",
    "        action_concat_itemid_dict[v] = count\n",
    "        count += 1\n",
    "# inverse\n",
    "inverse_action_concat_itemid_dict = dict(map(reversed, action_concat_itemid_dict.items()))\n",
    "\n",
    "adventure_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'Adventure' in itemid_genres_dict[v]:\n",
    "        adventure_concat_itemid_dict[v] = count\n",
    "        count += 1\n",
    "# inverse\n",
    "inverse_adventure_concat_itemid_dict = dict(map(reversed, adventure_concat_itemid_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれにアクションしていないユーザを削る\n",
    "# 全ユーザと、削ったあとでの対応関係を辞書として持っておく\n",
    "action_train_selected = action_train[action_train.getnnz(1)>0]\n",
    "adventure_train_selected = adventure_train[adventure_train.getnnz(1)>0]\n",
    "\n",
    "action_train_action_users = {}\n",
    "action_users = action_train.getnnz(1)>0\n",
    "count = 0\n",
    "for i in range(action_train.shape[0]):\n",
    "    if action_users[i]:\n",
    "        action_train_action_users[i] = count\n",
    "        count += 1\n",
    "\n",
    "# inverse\n",
    "inverse_action_train_action_users = dict(map(reversed, action_train_action_users.items()))\n",
    "\n",
    "adventure_train_action_users = {}\n",
    "adventure_users = adventure_train.getnnz(1)>0\n",
    "count = 0\n",
    "for i in range(adventure_train.shape[0]):\n",
    "    if adventure_users[i]:\n",
    "        adventure_train_action_users[i] = count\n",
    "        count += 1\n",
    "\n",
    "# inverse\n",
    "inverse_adventure_train_action_users = dict(map(reversed, adventure_train_action_users.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれでALSする\n",
    "# 今回は mediateでやったときのものを使う\n",
    "action_ALS = cloudpickle.load(open('output/ML-20M-action_ALS.pkl', 'rb'))\n",
    "adventure_ALS = cloudpickle.load(open(\"output/ML-20M-adventure_ALS.pkl\",\"rb\"))\n",
    "\n",
    "action_ALS_user_vectors = action_ALS.user_factors\n",
    "adventure_ALS_user_vectors = adventure_ALS.user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138389/138389 [00:01<00:00, 80989.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# actionとadventureでoverlapしているユーザで、ベクトルの対応表を作る\n",
    "overlap_action_user_vectors = []\n",
    "overlap_adventure_user_vectors = []\n",
    "count = 0\n",
    "for u in tqdm(range(X_train.shape[0])):\n",
    "    if u in action_train_action_users and u in adventure_train_action_users:\n",
    "        overlap_action_user_vectors.append(action_ALS_user_vectors[action_train_action_users[u]].tolist())\n",
    "        overlap_adventure_user_vectors.append(adventure_ALS_user_vectors[adventure_train_action_users[u]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fufufukakaka/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# AutoEncoderの学習をする\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "\n",
    "def build_model(input_dim, output_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation='relu')(inputs)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(output_dim, activation='sigmoid')(decoded)\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae','mse'])\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61805 samples, validate on 20602 samples\n",
      "Epoch 1/100\n",
      "61805/61805 [==============================] - 1s 23us/step - loss: 0.3708 - mae: 0.3826 - mse: 0.3708 - val_loss: 0.3458 - val_mae: 0.3574 - val_mse: 0.3458\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34584, saving model to output/ml-20m-model.h5\n",
      "Epoch 2/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.3463 - mae: 0.3570 - mse: 0.3463 - val_loss: 0.3390 - val_mae: 0.3540 - val_mse: 0.3390\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34584 to 0.33902, saving model to output/ml-20m-model.h5\n",
      "Epoch 3/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.3365 - mae: 0.3526 - mse: 0.3365 - val_loss: 0.3267 - val_mae: 0.3487 - val_mse: 0.3267\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33902 to 0.32669, saving model to output/ml-20m-model.h5\n",
      "Epoch 4/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.3193 - mae: 0.3448 - mse: 0.3193 - val_loss: 0.3093 - val_mae: 0.3398 - val_mse: 0.3093\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32669 to 0.30927, saving model to output/ml-20m-model.h5\n",
      "Epoch 5/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.3075 - mae: 0.3379 - mse: 0.3075 - val_loss: 0.3008 - val_mae: 0.3342 - val_mse: 0.3008\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30927 to 0.30083, saving model to output/ml-20m-model.h5\n",
      "Epoch 6/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.3002 - mae: 0.3328 - mse: 0.3002 - val_loss: 0.2950 - val_mae: 0.3301 - val_mse: 0.2950\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30083 to 0.29502, saving model to output/ml-20m-model.h5\n",
      "Epoch 7/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2951 - mae: 0.3290 - mse: 0.2951 - val_loss: 0.2910 - val_mae: 0.3269 - val_mse: 0.2910\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.29502 to 0.29096, saving model to output/ml-20m-model.h5\n",
      "Epoch 8/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2918 - mae: 0.3265 - mse: 0.2918 - val_loss: 0.2886 - val_mae: 0.3251 - val_mse: 0.2886\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.29096 to 0.28856, saving model to output/ml-20m-model.h5\n",
      "Epoch 9/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2896 - mae: 0.3247 - mse: 0.2896 - val_loss: 0.2862 - val_mae: 0.3230 - val_mse: 0.2862\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.28856 to 0.28621, saving model to output/ml-20m-model.h5\n",
      "Epoch 10/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2870 - mae: 0.3229 - mse: 0.2870 - val_loss: 0.2843 - val_mae: 0.3216 - val_mse: 0.2843\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.28621 to 0.28429, saving model to output/ml-20m-model.h5\n",
      "Epoch 11/100\n",
      "61805/61805 [==============================] - 1s 20us/step - loss: 0.2851 - mae: 0.3214 - mse: 0.2851 - val_loss: 0.2830 - val_mae: 0.3204 - val_mse: 0.2830\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.28429 to 0.28305, saving model to output/ml-20m-model.h5\n",
      "Epoch 12/100\n",
      "61805/61805 [==============================] - 1s 20us/step - loss: 0.2831 - mae: 0.3200 - mse: 0.2831 - val_loss: 0.2808 - val_mae: 0.3192 - val_mse: 0.2808\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.28305 to 0.28077, saving model to output/ml-20m-model.h5\n",
      "Epoch 13/100\n",
      "61805/61805 [==============================] - 1s 20us/step - loss: 0.2817 - mae: 0.3191 - mse: 0.2817 - val_loss: 0.2801 - val_mae: 0.3183 - val_mse: 0.2801\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.28077 to 0.28008, saving model to output/ml-20m-model.h5\n",
      "Epoch 14/100\n",
      "61805/61805 [==============================] - 1s 20us/step - loss: 0.2804 - mae: 0.3180 - mse: 0.2804 - val_loss: 0.2788 - val_mae: 0.3174 - val_mse: 0.2788\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.28008 to 0.27882, saving model to output/ml-20m-model.h5\n",
      "Epoch 15/100\n",
      "61805/61805 [==============================] - 1s 20us/step - loss: 0.2794 - mae: 0.3172 - mse: 0.2794 - val_loss: 0.2781 - val_mae: 0.3169 - val_mse: 0.2781\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.27882 to 0.27811, saving model to output/ml-20m-model.h5\n",
      "Epoch 16/100\n",
      "61805/61805 [==============================] - 1s 20us/step - loss: 0.2787 - mae: 0.3166 - mse: 0.2787 - val_loss: 0.2774 - val_mae: 0.3161 - val_mse: 0.2774\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.27811 to 0.27736, saving model to output/ml-20m-model.h5\n",
      "Epoch 17/100\n",
      "61805/61805 [==============================] - 1s 20us/step - loss: 0.2780 - mae: 0.3160 - mse: 0.2780 - val_loss: 0.2769 - val_mae: 0.3158 - val_mse: 0.2769\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.27736 to 0.27693, saving model to output/ml-20m-model.h5\n",
      "Epoch 18/100\n",
      "61805/61805 [==============================] - 1s 19us/step - loss: 0.2774 - mae: 0.3154 - mse: 0.2774 - val_loss: 0.2762 - val_mae: 0.3151 - val_mse: 0.2762\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.27693 to 0.27624, saving model to output/ml-20m-model.h5\n",
      "Epoch 19/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2768 - mae: 0.3149 - mse: 0.2768 - val_loss: 0.2760 - val_mae: 0.3150 - val_mse: 0.2760\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.27624 to 0.27603, saving model to output/ml-20m-model.h5\n",
      "Epoch 20/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2763 - mae: 0.3144 - mse: 0.2763 - val_loss: 0.2755 - val_mae: 0.3144 - val_mse: 0.2755\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.27603 to 0.27554, saving model to output/ml-20m-model.h5\n",
      "Epoch 21/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2755 - mae: 0.3140 - mse: 0.2755 - val_loss: 0.2749 - val_mae: 0.3144 - val_mse: 0.2749\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.27554 to 0.27494, saving model to output/ml-20m-model.h5\n",
      "Epoch 22/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2747 - mae: 0.3134 - mse: 0.2747 - val_loss: 0.2741 - val_mae: 0.3134 - val_mse: 0.2741\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.27494 to 0.27412, saving model to output/ml-20m-model.h5\n",
      "Epoch 23/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2742 - mae: 0.3130 - mse: 0.2742 - val_loss: 0.2737 - val_mae: 0.3127 - val_mse: 0.2737\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.27412 to 0.27369, saving model to output/ml-20m-model.h5\n",
      "Epoch 24/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2738 - mae: 0.3125 - mse: 0.2738 - val_loss: 0.2732 - val_mae: 0.3124 - val_mse: 0.2732\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.27369 to 0.27323, saving model to output/ml-20m-model.h5\n",
      "Epoch 25/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2732 - mae: 0.3119 - mse: 0.2732 - val_loss: 0.2732 - val_mae: 0.3123 - val_mse: 0.2732\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.27323 to 0.27316, saving model to output/ml-20m-model.h5\n",
      "Epoch 26/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2728 - mae: 0.3115 - mse: 0.2728 - val_loss: 0.2729 - val_mae: 0.3121 - val_mse: 0.2729\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.27316 to 0.27289, saving model to output/ml-20m-model.h5\n",
      "Epoch 27/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2724 - mae: 0.3112 - mse: 0.2724 - val_loss: 0.2724 - val_mae: 0.3113 - val_mse: 0.2724\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.27289 to 0.27238, saving model to output/ml-20m-model.h5\n",
      "Epoch 28/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2721 - mae: 0.3108 - mse: 0.2721 - val_loss: 0.2722 - val_mae: 0.3114 - val_mse: 0.2722\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.27238 to 0.27219, saving model to output/ml-20m-model.h5\n",
      "Epoch 29/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2717 - mae: 0.3105 - mse: 0.2717 - val_loss: 0.2715 - val_mae: 0.3107 - val_mse: 0.2715\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.27219 to 0.27153, saving model to output/ml-20m-model.h5\n",
      "Epoch 30/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2713 - mae: 0.3101 - mse: 0.2713 - val_loss: 0.2718 - val_mae: 0.3107 - val_mse: 0.2718\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.27153\n",
      "Epoch 31/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2710 - mae: 0.3098 - mse: 0.2710 - val_loss: 0.2715 - val_mae: 0.3106 - val_mse: 0.2715\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.27153 to 0.27152, saving model to output/ml-20m-model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2707 - mae: 0.3095 - mse: 0.2707 - val_loss: 0.2712 - val_mae: 0.3101 - val_mse: 0.2712\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.27152 to 0.27118, saving model to output/ml-20m-model.h5\n",
      "Epoch 33/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2705 - mae: 0.3092 - mse: 0.2705 - val_loss: 0.2711 - val_mae: 0.3101 - val_mse: 0.2711\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.27118 to 0.27106, saving model to output/ml-20m-model.h5\n",
      "Epoch 34/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2702 - mae: 0.3089 - mse: 0.2702 - val_loss: 0.2707 - val_mae: 0.3096 - val_mse: 0.2707\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.27106 to 0.27067, saving model to output/ml-20m-model.h5\n",
      "Epoch 35/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2700 - mae: 0.3087 - mse: 0.2700 - val_loss: 0.2710 - val_mae: 0.3099 - val_mse: 0.2710\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.27067\n",
      "Epoch 36/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2698 - mae: 0.3086 - mse: 0.2698 - val_loss: 0.2707 - val_mae: 0.3095 - val_mse: 0.2707\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.27067\n",
      "Epoch 37/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2696 - mae: 0.3084 - mse: 0.2696 - val_loss: 0.2706 - val_mae: 0.3095 - val_mse: 0.2706\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.27067 to 0.27062, saving model to output/ml-20m-model.h5\n",
      "Epoch 38/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2694 - mae: 0.3082 - mse: 0.2694 - val_loss: 0.2702 - val_mae: 0.3090 - val_mse: 0.2702\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.27062 to 0.27024, saving model to output/ml-20m-model.h5\n",
      "Epoch 39/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2693 - mae: 0.3080 - mse: 0.2693 - val_loss: 0.2702 - val_mae: 0.3088 - val_mse: 0.2702\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.27024 to 0.27016, saving model to output/ml-20m-model.h5\n",
      "Epoch 40/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2690 - mae: 0.3077 - mse: 0.2690 - val_loss: 0.2701 - val_mae: 0.3086 - val_mse: 0.2701\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.27016 to 0.27009, saving model to output/ml-20m-model.h5\n",
      "Epoch 41/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2689 - mae: 0.3076 - mse: 0.2689 - val_loss: 0.2699 - val_mae: 0.3086 - val_mse: 0.2699\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.27009 to 0.26993, saving model to output/ml-20m-model.h5\n",
      "Epoch 42/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2688 - mae: 0.3075 - mse: 0.2688 - val_loss: 0.2698 - val_mae: 0.3084 - val_mse: 0.2698\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.26993 to 0.26976, saving model to output/ml-20m-model.h5\n",
      "Epoch 43/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2686 - mae: 0.3073 - mse: 0.2686 - val_loss: 0.2698 - val_mae: 0.3083 - val_mse: 0.2698\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.26976 to 0.26976, saving model to output/ml-20m-model.h5\n",
      "Epoch 44/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2684 - mae: 0.3070 - mse: 0.2684 - val_loss: 0.2697 - val_mae: 0.3082 - val_mse: 0.2697\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.26976 to 0.26966, saving model to output/ml-20m-model.h5\n",
      "Epoch 45/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2683 - mae: 0.3070 - mse: 0.2683 - val_loss: 0.2696 - val_mae: 0.3081 - val_mse: 0.2696\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.26966 to 0.26958, saving model to output/ml-20m-model.h5\n",
      "Epoch 46/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2682 - mae: 0.3068 - mse: 0.2682 - val_loss: 0.2698 - val_mae: 0.3083 - val_mse: 0.2698\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.26958\n",
      "Epoch 47/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2680 - mae: 0.3066 - mse: 0.2680 - val_loss: 0.2697 - val_mae: 0.3081 - val_mse: 0.2697\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.26958\n",
      "Epoch 48/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2679 - mae: 0.3066 - mse: 0.2679 - val_loss: 0.2696 - val_mae: 0.3081 - val_mse: 0.2696\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.26958 to 0.26958, saving model to output/ml-20m-model.h5\n",
      "Epoch 49/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2677 - mae: 0.3063 - mse: 0.2677 - val_loss: 0.2692 - val_mae: 0.3077 - val_mse: 0.2692\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.26958 to 0.26925, saving model to output/ml-20m-model.h5\n",
      "Epoch 50/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2676 - mae: 0.3063 - mse: 0.2676 - val_loss: 0.2694 - val_mae: 0.3077 - val_mse: 0.2694\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.26925\n",
      "Epoch 51/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2675 - mae: 0.3062 - mse: 0.2675 - val_loss: 0.2693 - val_mae: 0.3076 - val_mse: 0.2693\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.26925\n",
      "Epoch 52/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2674 - mae: 0.3060 - mse: 0.2674 - val_loss: 0.2689 - val_mae: 0.3073 - val_mse: 0.2689\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.26925 to 0.26894, saving model to output/ml-20m-model.h5\n",
      "Epoch 53/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2674 - mae: 0.3060 - mse: 0.2674 - val_loss: 0.2691 - val_mae: 0.3077 - val_mse: 0.2691\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.26894\n",
      "Epoch 54/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2673 - mae: 0.3059 - mse: 0.2673 - val_loss: 0.2690 - val_mae: 0.3072 - val_mse: 0.2690\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.26894\n",
      "Epoch 55/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2672 - mae: 0.3058 - mse: 0.2672 - val_loss: 0.2692 - val_mae: 0.3074 - val_mse: 0.2692\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.26894\n",
      "Epoch 56/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2671 - mae: 0.3057 - mse: 0.2671 - val_loss: 0.2689 - val_mae: 0.3070 - val_mse: 0.2689\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.26894 to 0.26889, saving model to output/ml-20m-model.h5\n",
      "Epoch 57/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2669 - mae: 0.3055 - mse: 0.2669 - val_loss: 0.2688 - val_mae: 0.3070 - val_mse: 0.2688\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.26889 to 0.26884, saving model to output/ml-20m-model.h5\n",
      "Epoch 58/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2667 - mae: 0.3053 - mse: 0.2667 - val_loss: 0.2689 - val_mae: 0.3070 - val_mse: 0.2689\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.26884\n",
      "Epoch 59/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2666 - mae: 0.3052 - mse: 0.2666 - val_loss: 0.2690 - val_mae: 0.3072 - val_mse: 0.2690\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.26884\n",
      "Epoch 60/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2666 - mae: 0.3052 - mse: 0.2666 - val_loss: 0.2687 - val_mae: 0.3068 - val_mse: 0.2687\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.26884 to 0.26870, saving model to output/ml-20m-model.h5\n",
      "Epoch 61/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2665 - mae: 0.3051 - mse: 0.2665 - val_loss: 0.2687 - val_mae: 0.3067 - val_mse: 0.2687\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.26870 to 0.26866, saving model to output/ml-20m-model.h5\n",
      "Epoch 62/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2663 - mae: 0.3049 - mse: 0.2663 - val_loss: 0.2687 - val_mae: 0.3068 - val_mse: 0.2687\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.26866\n",
      "Epoch 63/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2662 - mae: 0.3048 - mse: 0.2662 - val_loss: 0.2684 - val_mae: 0.3063 - val_mse: 0.2684\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.26866 to 0.26839, saving model to output/ml-20m-model.h5\n",
      "Epoch 64/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2661 - mae: 0.3047 - mse: 0.2661 - val_loss: 0.2685 - val_mae: 0.3064 - val_mse: 0.2685\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.26839\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2661 - mae: 0.3047 - mse: 0.2661 - val_loss: 0.2685 - val_mae: 0.3065 - val_mse: 0.2685\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.26839\n",
      "Epoch 66/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2659 - mae: 0.3045 - mse: 0.2659 - val_loss: 0.2684 - val_mae: 0.3064 - val_mse: 0.2684\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.26839\n",
      "Epoch 67/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2659 - mae: 0.3045 - mse: 0.2659 - val_loss: 0.2684 - val_mae: 0.3064 - val_mse: 0.2684\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.26839\n",
      "Epoch 68/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2658 - mae: 0.3043 - mse: 0.2658 - val_loss: 0.2684 - val_mae: 0.3062 - val_mse: 0.2684\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.26839\n",
      "Epoch 69/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2657 - mae: 0.3043 - mse: 0.2657 - val_loss: 0.2684 - val_mae: 0.3062 - val_mse: 0.2684\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.26839 to 0.26837, saving model to output/ml-20m-model.h5\n",
      "Epoch 70/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2656 - mae: 0.3041 - mse: 0.2656 - val_loss: 0.2681 - val_mae: 0.3059 - val_mse: 0.2681\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.26837 to 0.26812, saving model to output/ml-20m-model.h5\n",
      "Epoch 71/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2655 - mae: 0.3040 - mse: 0.2655 - val_loss: 0.2679 - val_mae: 0.3058 - val_mse: 0.2679\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.26812 to 0.26792, saving model to output/ml-20m-model.h5\n",
      "Epoch 72/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2655 - mae: 0.3040 - mse: 0.2655 - val_loss: 0.2683 - val_mae: 0.3061 - val_mse: 0.2683\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.26792\n",
      "Epoch 73/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2654 - mae: 0.3039 - mse: 0.2654 - val_loss: 0.2681 - val_mae: 0.3060 - val_mse: 0.2681\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.26792\n",
      "Epoch 74/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2653 - mae: 0.3038 - mse: 0.2653 - val_loss: 0.2682 - val_mae: 0.3059 - val_mse: 0.2682\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.26792\n",
      "Epoch 75/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2653 - mae: 0.3038 - mse: 0.2653 - val_loss: 0.2680 - val_mae: 0.3057 - val_mse: 0.2680\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.26792\n",
      "Epoch 76/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2652 - mae: 0.3038 - mse: 0.2652 - val_loss: 0.2680 - val_mae: 0.3058 - val_mse: 0.2680\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.26792\n",
      "Epoch 77/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2651 - mae: 0.3037 - mse: 0.2651 - val_loss: 0.2679 - val_mae: 0.3055 - val_mse: 0.2679\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.26792 to 0.26789, saving model to output/ml-20m-model.h5\n",
      "Epoch 78/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2651 - mae: 0.3036 - mse: 0.2651 - val_loss: 0.2680 - val_mae: 0.3057 - val_mse: 0.2680\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.26789\n",
      "Epoch 79/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2650 - mae: 0.3036 - mse: 0.2650 - val_loss: 0.2678 - val_mae: 0.3055 - val_mse: 0.2678\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.26789 to 0.26782, saving model to output/ml-20m-model.h5\n",
      "Epoch 80/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2650 - mae: 0.3035 - mse: 0.2650 - val_loss: 0.2678 - val_mae: 0.3054 - val_mse: 0.2678\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.26782 to 0.26781, saving model to output/ml-20m-model.h5\n",
      "Epoch 81/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2649 - mae: 0.3034 - mse: 0.2649 - val_loss: 0.2677 - val_mae: 0.3053 - val_mse: 0.2677\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.26781 to 0.26772, saving model to output/ml-20m-model.h5\n",
      "Epoch 82/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2649 - mae: 0.3034 - mse: 0.2649 - val_loss: 0.2678 - val_mae: 0.3054 - val_mse: 0.2678\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.26772\n",
      "Epoch 83/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2648 - mae: 0.3033 - mse: 0.2648 - val_loss: 0.2680 - val_mae: 0.3059 - val_mse: 0.2680\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.26772\n",
      "Epoch 84/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2647 - mae: 0.3033 - mse: 0.2647 - val_loss: 0.2679 - val_mae: 0.3055 - val_mse: 0.2679\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.26772\n",
      "Epoch 85/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2647 - mae: 0.3032 - mse: 0.2647 - val_loss: 0.2680 - val_mae: 0.3055 - val_mse: 0.2680\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.26772\n",
      "Epoch 86/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2646 - mae: 0.3032 - mse: 0.2646 - val_loss: 0.2678 - val_mae: 0.3056 - val_mse: 0.2678\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.26772\n",
      "Epoch 87/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2646 - mae: 0.3031 - mse: 0.2646 - val_loss: 0.2679 - val_mae: 0.3053 - val_mse: 0.2679\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.26772\n",
      "Epoch 88/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2645 - mae: 0.3031 - mse: 0.2645 - val_loss: 0.2677 - val_mae: 0.3054 - val_mse: 0.2677\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.26772 to 0.26769, saving model to output/ml-20m-model.h5\n",
      "Epoch 89/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2645 - mae: 0.3030 - mse: 0.2645 - val_loss: 0.2678 - val_mae: 0.3053 - val_mse: 0.2678\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.26769\n",
      "Epoch 90/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2645 - mae: 0.3030 - mse: 0.2645 - val_loss: 0.2677 - val_mae: 0.3054 - val_mse: 0.2677\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.26769\n",
      "Epoch 91/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2643 - mae: 0.3028 - mse: 0.2643 - val_loss: 0.2677 - val_mae: 0.3051 - val_mse: 0.2677\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.26769 to 0.26766, saving model to output/ml-20m-model.h5\n",
      "Epoch 92/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 0.2643 - mae: 0.3028 - mse: 0.2643 - val_loss: 0.2677 - val_mae: 0.3055 - val_mse: 0.2677\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.26766\n",
      "Epoch 93/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2643 - mae: 0.3028 - mse: 0.2643 - val_loss: 0.2675 - val_mae: 0.3050 - val_mse: 0.2675\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.26766 to 0.26753, saving model to output/ml-20m-model.h5\n",
      "Epoch 94/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2642 - mae: 0.3027 - mse: 0.2642 - val_loss: 0.2674 - val_mae: 0.3049 - val_mse: 0.2674\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.26753 to 0.26740, saving model to output/ml-20m-model.h5\n",
      "Epoch 95/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2641 - mae: 0.3027 - mse: 0.2641 - val_loss: 0.2677 - val_mae: 0.3051 - val_mse: 0.2677\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.26740\n",
      "Epoch 96/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2642 - mae: 0.3027 - mse: 0.2642 - val_loss: 0.2675 - val_mae: 0.3049 - val_mse: 0.2675\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.26740\n",
      "Epoch 97/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2641 - mae: 0.3026 - mse: 0.2641 - val_loss: 0.2678 - val_mae: 0.3053 - val_mse: 0.2678\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.26740\n",
      "Epoch 98/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2641 - mae: 0.3026 - mse: 0.2641 - val_loss: 0.2675 - val_mae: 0.3049 - val_mse: 0.2675\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.26740\n",
      "Epoch 99/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2640 - mae: 0.3025 - mse: 0.2640 - val_loss: 0.2677 - val_mae: 0.3050 - val_mse: 0.2677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00099: val_loss did not improve from 0.26740\n",
      "Epoch 100/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 0.2639 - mae: 0.3025 - mse: 0.2639 - val_loss: 0.2674 - val_mae: 0.3049 - val_mse: 0.2674\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.26740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc9ac46bf28>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_vector, X_test_vector, y_train_vector, y_test_vector = train_test_split(overlap_action_user_vectors, overlap_adventure_user_vectors, random_state=42)\n",
    "X_train_vector, X_val_vector, y_train_vector, y_val_vector = train_test_split(X_train_vector, y_train_vector, random_state=42)\n",
    "\n",
    "epoch_size = 100\n",
    "batch_size = 256\n",
    "mcheck = ModelCheckpoint(\n",
    "    'output/ml-20m-model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "es_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "model = build_model(np.array(X_train_vector).shape[1], np.array(y_train_vector).shape[1])\n",
    "model.fit(\n",
    "    np.array(X_train_vector),\n",
    "    np.array(y_train_vector),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_size,\n",
    "    validation_data=(\n",
    "        np.array(X_val_vector),\n",
    "        np.array(y_val_vector)),\n",
    "    callbacks=[\n",
    "        mcheck,\n",
    "        es_cb],\n",
    "    shuffle=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.5176917184448359\n"
     ]
    }
   ],
   "source": [
    "# テストデータに対するRMSE計算\n",
    "from sklearn.metrics import mean_squared_error\n",
    "best_model = load_model('output/ml-20m-model.h5')\n",
    "y_pred = best_model.predict(np.array(X_test_vector))\n",
    "rmse_ = np.sqrt(mean_squared_error(y_pred, np.array(y_test_vector)))\n",
    "print('rmse: {}'.format(rmse_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaeとで、rmseを比較し、良い方を選ぶ\n",
    "# ref. https://keras.io/examples/variational_autoencoder/\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "class VAE():\n",
    "    def __init__(self, input_dim, intermediate_dim, latent_dim, original_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.original_dim = original_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.z_mean = None\n",
    "        self.z_log_var = None\n",
    "\n",
    "\n",
    "    # reparameterization trick\n",
    "    # instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "    # z = z_mean + sqrt(var) * epsilon\n",
    "    def sampling(self, args):\n",
    "        \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "        # Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "        # Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean = 0 and std = 1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "    def vae_loss(self, y_true, y_pred):\n",
    "        reconstruction_loss = mse(y_true, y_pred)\n",
    "        reconstruction_loss *= self.original_dim\n",
    "        kl_loss = 1 + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        return vae_loss\n",
    "\n",
    "\n",
    "    def build_vae(self):\n",
    "        # VAE model = encoder + decoder\n",
    "        # build encoder model\n",
    "        inputs = Input(shape=(self.input_dim,), name='encoder_input')\n",
    "        x = Dense(128, activation='relu')(inputs)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        self.z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        self.z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "\n",
    "        # use reparameterization trick to push the sampling out as input\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(self.sampling, output_shape=(self.latent_dim,), name='z')([self.z_mean, self.z_log_var])\n",
    "\n",
    "        # instantiate encoder model\n",
    "        encoder = Model(inputs, [self.z_mean, self.z_log_var, z], name='encoder')\n",
    "\n",
    "        # build decoder model\n",
    "        latent_inputs = Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        x = Dense(32, activation='relu')(latent_inputs)\n",
    "        x = Dense(64, activation='relu')(latent_inputs)\n",
    "        x = Dense(128, activation='relu')(latent_inputs)\n",
    "        decoder_outputs = Dense(self.original_dim, activation='sigmoid')(x)\n",
    "\n",
    "        # instantiate decoder model\n",
    "        decoder = Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "        # instantiate VAE model\n",
    "        outputs = decoder(encoder(inputs)[2])\n",
    "        vae = Model(inputs, outputs, name='vae')\n",
    "        vae.compile(optimizer='adam', loss=self.vae_loss)\n",
    "        return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(np.array(X_train_vector).shape[1], 256, 2, np.array(y_train_vector).shape[1])\n",
    "model = vae.build_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 23396     \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 100)               13284     \n",
      "=================================================================\n",
      "Total params: 36,680\n",
      "Trainable params: 36,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61805 samples, validate on 20602 samples\n",
      "Epoch 1/100\n",
      "61805/61805 [==============================] - 2s 30us/step - loss: 40.2766 - val_loss: 34.3387\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 34.33868, saving model to output/ml-20m-vae.h5\n",
      "Epoch 2/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 34.0882 - val_loss: 33.3679\n",
      "\n",
      "Epoch 00002: val_loss improved from 34.33868 to 33.36791, saving model to output/ml-20m-vae.h5\n",
      "Epoch 3/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 33.3053 - val_loss: 32.7922\n",
      "\n",
      "Epoch 00003: val_loss improved from 33.36791 to 32.79220, saving model to output/ml-20m-vae.h5\n",
      "Epoch 4/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.9902 - val_loss: 32.6189\n",
      "\n",
      "Epoch 00004: val_loss improved from 32.79220 to 32.61888, saving model to output/ml-20m-vae.h5\n",
      "Epoch 5/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.8649 - val_loss: 32.5137\n",
      "\n",
      "Epoch 00005: val_loss improved from 32.61888 to 32.51373, saving model to output/ml-20m-vae.h5\n",
      "Epoch 6/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.7853 - val_loss: 32.4430\n",
      "\n",
      "Epoch 00006: val_loss improved from 32.51373 to 32.44305, saving model to output/ml-20m-vae.h5\n",
      "Epoch 7/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.7313 - val_loss: 32.4250\n",
      "\n",
      "Epoch 00007: val_loss improved from 32.44305 to 32.42496, saving model to output/ml-20m-vae.h5\n",
      "Epoch 8/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.6738 - val_loss: 32.3642\n",
      "\n",
      "Epoch 00008: val_loss improved from 32.42496 to 32.36423, saving model to output/ml-20m-vae.h5\n",
      "Epoch 9/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.6214 - val_loss: 32.3060\n",
      "\n",
      "Epoch 00009: val_loss improved from 32.36423 to 32.30604, saving model to output/ml-20m-vae.h5\n",
      "Epoch 10/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.5727 - val_loss: 32.2710\n",
      "\n",
      "Epoch 00010: val_loss improved from 32.30604 to 32.27100, saving model to output/ml-20m-vae.h5\n",
      "Epoch 11/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.5325 - val_loss: 32.2371\n",
      "\n",
      "Epoch 00011: val_loss improved from 32.27100 to 32.23710, saving model to output/ml-20m-vae.h5\n",
      "Epoch 12/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.5137 - val_loss: 32.2375\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 32.23710\n",
      "Epoch 13/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.4913 - val_loss: 32.2637\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 32.23710\n",
      "Epoch 14/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.4782 - val_loss: 32.1924\n",
      "\n",
      "Epoch 00014: val_loss improved from 32.23710 to 32.19243, saving model to output/ml-20m-vae.h5\n",
      "Epoch 15/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.4678 - val_loss: 32.1766\n",
      "\n",
      "Epoch 00015: val_loss improved from 32.19243 to 32.17656, saving model to output/ml-20m-vae.h5\n",
      "Epoch 16/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.4621 - val_loss: 32.1774\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 32.17656\n",
      "Epoch 17/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.4507 - val_loss: 32.1679\n",
      "\n",
      "Epoch 00017: val_loss improved from 32.17656 to 32.16786, saving model to output/ml-20m-vae.h5\n",
      "Epoch 18/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.4293 - val_loss: 32.1486\n",
      "\n",
      "Epoch 00018: val_loss improved from 32.16786 to 32.14858, saving model to output/ml-20m-vae.h5\n",
      "Epoch 19/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.4176 - val_loss: 32.1563\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 32.14858\n",
      "Epoch 20/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3997 - val_loss: 32.1467\n",
      "\n",
      "Epoch 00020: val_loss improved from 32.14858 to 32.14672, saving model to output/ml-20m-vae.h5\n",
      "Epoch 21/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3995 - val_loss: 32.1583\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 32.14672\n",
      "Epoch 22/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3885 - val_loss: 32.1377\n",
      "\n",
      "Epoch 00022: val_loss improved from 32.14672 to 32.13770, saving model to output/ml-20m-vae.h5\n",
      "Epoch 23/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3789 - val_loss: 32.1284\n",
      "\n",
      "Epoch 00023: val_loss improved from 32.13770 to 32.12844, saving model to output/ml-20m-vae.h5\n",
      "Epoch 24/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3721 - val_loss: 32.1249\n",
      "\n",
      "Epoch 00024: val_loss improved from 32.12844 to 32.12487, saving model to output/ml-20m-vae.h5\n",
      "Epoch 25/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3578 - val_loss: 32.1068\n",
      "\n",
      "Epoch 00025: val_loss improved from 32.12487 to 32.10683, saving model to output/ml-20m-vae.h5\n",
      "Epoch 26/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3557 - val_loss: 32.1137\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 32.10683\n",
      "Epoch 27/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.3439 - val_loss: 32.1381\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 32.10683\n",
      "Epoch 28/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3390 - val_loss: 32.1033\n",
      "\n",
      "Epoch 00028: val_loss improved from 32.10683 to 32.10334, saving model to output/ml-20m-vae.h5\n",
      "Epoch 29/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.3238 - val_loss: 32.1123\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 32.10334\n",
      "Epoch 30/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3273 - val_loss: 32.0704\n",
      "\n",
      "Epoch 00030: val_loss improved from 32.10334 to 32.07036, saving model to output/ml-20m-vae.h5\n",
      "Epoch 31/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3155 - val_loss: 32.0831\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 32.07036\n",
      "Epoch 32/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3142 - val_loss: 32.0838\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 32.07036\n",
      "Epoch 33/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.3075 - val_loss: 32.0883\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 32.07036\n",
      "Epoch 34/100\n",
      "61805/61805 [==============================] - 1s 17us/step - loss: 32.3131 - val_loss: 32.0496\n",
      "\n",
      "Epoch 00034: val_loss improved from 32.07036 to 32.04959, saving model to output/ml-20m-vae.h5\n",
      "Epoch 35/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.2927 - val_loss: 32.0625\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 32.04959\n",
      "Epoch 36/100\n",
      "61805/61805 [==============================] - 1s 19us/step - loss: 32.2776 - val_loss: 32.0108\n",
      "\n",
      "Epoch 00036: val_loss improved from 32.04959 to 32.01083, saving model to output/ml-20m-vae.h5\n",
      "Epoch 37/100\n",
      "61805/61805 [==============================] - 1s 20us/step - loss: 32.2550 - val_loss: 32.0552\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 32.01083\n",
      "Epoch 38/100\n",
      "61805/61805 [==============================] - 1s 19us/step - loss: 32.2647 - val_loss: 32.0484\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 32.01083\n",
      "Epoch 39/100\n",
      "61805/61805 [==============================] - 1s 19us/step - loss: 32.2461 - val_loss: 32.0325\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 32.01083\n",
      "Epoch 40/100\n",
      "61805/61805 [==============================] - 1s 19us/step - loss: 32.2469 - val_loss: 32.0456\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 32.01083\n",
      "Epoch 41/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.2400 - val_loss: 32.0203\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 32.01083\n",
      "Epoch 42/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.2365 - val_loss: 32.0232\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 32.01083\n",
      "Epoch 43/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.2275 - val_loss: 32.0320\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 32.01083\n",
      "Epoch 44/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.2278 - val_loss: 32.0244\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 32.01083\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.2294 - val_loss: 32.0323\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 32.01083\n",
      "Epoch 46/100\n",
      "61805/61805 [==============================] - 1s 18us/step - loss: 32.2209 - val_loss: 32.0129\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 32.01083\n",
      "Epoch 00046: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc9844d8c50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size = 100\n",
    "batch_size = 256\n",
    "mcheck = ModelCheckpoint(\n",
    "    'output/ml-20m-vae.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "es_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "model.fit(\n",
    "    np.array(X_train_vector),\n",
    "    np.array(y_train_vector),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_size,\n",
    "    validation_data=(\n",
    "        np.array(X_val_vector),\n",
    "        np.array(y_val_vector)),\n",
    "    callbacks=[\n",
    "        mcheck,\n",
    "        es_cb],\n",
    "    shuffle=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.5572697988473447\n"
     ]
    }
   ],
   "source": [
    "# テストデータに対するRMSE計算\n",
    "from sklearn.metrics import mean_squared_error\n",
    "best_model_vae = vae.build_vae()\n",
    "best_model_vae.load_weights('output/ml-20m-vae.h5')\n",
    "y_pred = best_model_vae.predict(np.array(X_test_vector))\n",
    "rmse_ = np.sqrt(mean_squared_error(y_pred, np.array(y_test_vector)))\n",
    "print('rmse: {}'.format(rmse_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(VAEのやりかたが悪かったようなだけな気もするが)今回は素のautoencoderを採用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習結果が固定できないので、10回学習した値の平均値を採用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_count: 0\n",
      "rmse: 0.5183830380815655\n",
      "learning_count: 1\n",
      "rmse: 0.5187530618319904\n",
      "learning_count: 2\n",
      "rmse: 0.5181147621281058\n",
      "learning_count: 3\n",
      "rmse: 0.5179883554745452\n",
      "learning_count: 4\n",
      "rmse: 0.5177950388480708\n",
      "learning_count: 5\n",
      "rmse: 0.51769281554704\n",
      "learning_count: 6\n",
      "rmse: 0.5166800007327981\n",
      "learning_count: 7\n",
      "rmse: 0.5181607680538545\n",
      "learning_count: 8\n",
      "rmse: 0.5173283245405115\n",
      "learning_count: 9\n",
      "rmse: 0.5167998675632172\n"
     ]
    }
   ],
   "source": [
    "X = np.array(overlap_action_user_vectors)\n",
    "y = np.array(overlap_adventure_user_vectors)\n",
    "epoch_size = 100\n",
    "batch_size = 256\n",
    "es_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode='auto')\n",
    "models = []\n",
    "rmses_ = []\n",
    "for learning_count in range(10):\n",
    "    print(f\"learning_count: {learning_count}\")\n",
    "    X_train_vector, X_test_vector, y_train_vector, y_test_vector = train_test_split(X, y, random_state=42)\n",
    "    X_train_vector, X_val_vector, y_train_vector, y_val_vector = train_test_split(X_train_vector, y_train_vector, random_state=42)\n",
    "    model = build_model(X_train_vector.shape[1], y_train_vector.shape[1])\n",
    "    mcheck = ModelCheckpoint(\n",
    "        f'output/ml-20m-model_{learning_count}.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_vector,\n",
    "        y_train_vector,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epoch_size,\n",
    "        validation_data=(\n",
    "            X_val_vector,\n",
    "            y_val_vector),\n",
    "        callbacks=[\n",
    "            mcheck,\n",
    "            es_cb],\n",
    "        shuffle=True,\n",
    "        verbose=0)\n",
    "    best_model = load_model(f'output/ml-20m-model_{learning_count}.h5')\n",
    "    y_pred = best_model.predict(X_test_vector)\n",
    "    rmse_ = np.sqrt(mean_squared_error(y_pred, y_test_vector))\n",
    "    print('rmse: {}'.format(rmse_))\n",
    "    rmses_.append(rmse_)\n",
    "    models.append(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138389/138389 [00:20<00:00, 6720.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# 評価対象のユーザ\n",
    "test_adventure_pos_items_dict = {}\n",
    "for i in tqdm(range(X_test.shape[0])):\n",
    "    # trainでadventureにアクションしていないユーザに\n",
    "    rated_items = X_train[i, :].indices\n",
    "    if len([v for v in rated_items if 'Adventure' in itemid_genres_dict[v]]) == 0:\n",
    "        # X_testの中でstoreしているアイテムが0以上のユーザに\n",
    "        if X_test[i, :].nnz > 0:\n",
    "            test_items = []\n",
    "            selected_user_ratings = X_test[i, :]\n",
    "            value_indices = selected_user_ratings.indices\n",
    "            sorted_indices = np.argsort(-X_test[i, :].toarray())[0]\n",
    "            # valueがあるアイテムのジャンルがadventureの場合に\n",
    "            for v in sorted_indices[:len(value_indices)]:\n",
    "                if 'Adventure' in itemid_genres_dict[v]:\n",
    "                    test_items.append(v)\n",
    "            if len(test_items) > 0:\n",
    "                test_adventure_pos_items_dict[i] = test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adventureのitemのベクトル\n",
    "adventure_item_vectors = adventure_ALS.item_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18523/18523 [01:13<00:00, 251.67it/s]\n",
      "100%|██████████| 18523/18523 [01:14<00:00, 248.33it/s]\n",
      "100%|██████████| 18523/18523 [01:16<00:00, 242.66it/s]\n",
      "100%|██████████| 18523/18523 [01:17<00:00, 239.73it/s]\n",
      "100%|██████████| 18523/18523 [01:17<00:00, 239.98it/s]\n",
      "100%|██████████| 18523/18523 [01:18<00:00, 235.97it/s]\n",
      "100%|██████████| 18523/18523 [01:21<00:00, 226.29it/s]\n",
      "100%|██████████| 18523/18523 [01:19<00:00, 231.56it/s]\n",
      "100%|██████████| 18523/18523 [01:21<00:00, 226.48it/s]\n",
      "100%|██████████| 18523/18523 [01:24<00:00, 220.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.recommend_util import ndcg\n",
    "\n",
    "ndcg_values = []\n",
    "for learning_count in range(10):\n",
    "    ndcgs = {\n",
    "        'ndcg5':  [],\n",
    "        'ndcg10':  [],\n",
    "        'ndcg20':  [],\n",
    "        'ndcg50':  [],\n",
    "        'ndcg100':  []\n",
    "    }\n",
    "    best_model = load_model(f'output/ml-20m-model_{learning_count}.h5')\n",
    "\n",
    "    for userid, pos_items in tqdm(test_adventure_pos_items_dict.items()):\n",
    "       # pos_itemsをadventure_matrixの次元に変換する\n",
    "        pos_items = np.array([adventure_concat_itemid_dict[v] for v in pos_items])\n",
    "        # useridに対応するユーザベクトル(action)を得る\n",
    "        try:\n",
    "            action_userid = action_train_action_users[userid]\n",
    "        except:\n",
    "            # 推薦できないユーザの場合は無条件で0を入れる\n",
    "            ndcgs['ndcg5'].append(0)\n",
    "            ndcgs['ndcg10'].append(0)\n",
    "            ndcgs['ndcg20'].append(0)\n",
    "            ndcgs['ndcg50'].append(0)\n",
    "            ndcgs['ndcg100'].append(0)\n",
    "            continue\n",
    "\n",
    "        action_user_vector = action_ALS_user_vectors[action_userid, :]\n",
    "        # autoencoderを使ってadventureの次元に変換する\n",
    "        adventure_user_vector_action_AE = best_model.predict(action_user_vector.reshape(1, -1))\n",
    "        # adventureのitemのベクトルと掛け合わせる\n",
    "        adv_predict = np.dot(adventure_user_vector_action_AE, adventure_item_vectors.T)\n",
    "        # sum_ratingsをargsort\n",
    "        sorted_indices = np.array([v for v in np.argsort(-adv_predict)])[0]\n",
    "        ndcgs['ndcg5'].append(ndcg(sorted_indices[:5], pos_items))\n",
    "        ndcgs['ndcg10'].append(ndcg(sorted_indices[:10], pos_items))\n",
    "        ndcgs['ndcg20'].append(ndcg(sorted_indices[:20], pos_items))\n",
    "        ndcgs['ndcg50'].append(ndcg(sorted_indices[:50], pos_items))\n",
    "        ndcgs['ndcg100'].append(ndcg(sorted_indices[:100], pos_items))\n",
    "    \n",
    "    ndcg_values.append(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@5: 0.13987014738496223\n",
      "ndcg@10: 0.1872858615974345\n",
      "ndcg@20: 0.23374553053393812\n",
      "ndcg@50: 0.30161603726932323\n",
      "ndcg@100: 0.34618679444756456\n"
     ]
    }
   ],
   "source": [
    "ndcg5 = []\n",
    "ndcg10 = []\n",
    "ndcg20 = []\n",
    "ndcg50 = []\n",
    "ndcg100 = []\n",
    "for ndcgs in  ndcg_values:\n",
    "    ndcg5.append(np.mean(ndcgs['ndcg5']))\n",
    "    ndcg10.append(np.mean(ndcgs['ndcg10']))\n",
    "    ndcg20.append(np.mean(ndcgs['ndcg20']))\n",
    "    ndcg50.append(np.mean(ndcgs['ndcg50']))\n",
    "    ndcg100.append(np.mean(ndcgs['ndcg100']))\n",
    "print(\"ndcg@5: {}\".format(np.mean(ndcg5)))\n",
    "print(\"ndcg@10: {}\".format(np.mean(ndcg10)))\n",
    "print(\"ndcg@20: {}\".format(np.mean(ndcg20)))\n",
    "print(\"ndcg@50: {}\".format(np.mean(ndcg50)))\n",
    "print(\"ndcg@100: {}\".format(np.mean(ndcg100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_count: 0\n",
      "rmse: 0.5251619330752034\n",
      "learning_count: 1\n",
      "rmse: 0.507233192190287\n",
      "learning_count: 2\n",
      "rmse: 0.5161260012470608\n",
      "learning_count: 3\n",
      "rmse: 0.5108282018275829\n",
      "learning_count: 4\n",
      "rmse: 0.5257376499367238\n",
      "learning_count: 5\n",
      "rmse: 0.5233868038857069\n",
      "learning_count: 6\n",
      "rmse: 0.5217946238073435\n",
      "learning_count: 7\n",
      "rmse: 0.5153032209228041\n",
      "learning_count: 8\n",
      "rmse: 0.5137775134628675\n",
      "learning_count: 9\n",
      "rmse: 0.5158392218917118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = np.array(overlap_action_user_vectors)\n",
    "y = np.array(overlap_adventure_user_vectors)\n",
    "epoch_size = 100\n",
    "batch_size = 256\n",
    "es_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode='auto')\n",
    "models = []\n",
    "rmses_ = []\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "count = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(f\"learning_count: {count}\")\n",
    "    count += 1\n",
    "    X_train_vector, X_test_vector = X[train_index], X[test_index]\n",
    "    y_train_vector, y_test_vector = y[train_index], y[test_index]\n",
    "    X_train_vector, X_val_vector, y_train_vector, y_val_vector = train_test_split(X_train_vector, y_train_vector, random_state=42)\n",
    "    model = build_model(X_train_vector.shape[1], y_train_vector.shape[1])\n",
    "    mcheck = ModelCheckpoint(\n",
    "        f'output/ml-20m-model_k_{count}.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_vector,\n",
    "        y_train_vector,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epoch_size,\n",
    "        validation_data=(\n",
    "            X_val_vector,\n",
    "            y_val_vector),\n",
    "        callbacks=[\n",
    "            mcheck,\n",
    "            es_cb],\n",
    "        shuffle=True,\n",
    "        verbose=0)\n",
    "    best_model = load_model(f'output/ml-20m-model_k_{count}.h5')\n",
    "    y_pred = best_model.predict(X_test_vector)\n",
    "    rmse_ = np.sqrt(mean_squared_error(y_pred, y_test_vector))\n",
    "    print('rmse: {}'.format(rmse_))\n",
    "    rmses_.append(rmse_)\n",
    "    models.append(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18523/18523 [01:01<00:00, 303.13it/s]\n",
      "100%|██████████| 18523/18523 [01:02<00:00, 297.20it/s]\n",
      "100%|██████████| 18523/18523 [01:03<00:00, 292.42it/s]\n",
      "100%|██████████| 18523/18523 [01:04<00:00, 288.73it/s]\n",
      "100%|██████████| 18523/18523 [01:04<00:00, 285.32it/s]\n",
      "100%|██████████| 18523/18523 [01:05<00:00, 282.67it/s]\n",
      "100%|██████████| 18523/18523 [01:05<00:00, 283.13it/s]\n",
      "100%|██████████| 18523/18523 [01:07<00:00, 274.51it/s]\n",
      "100%|██████████| 18523/18523 [01:08<00:00, 271.65it/s]\n",
      "100%|██████████| 18523/18523 [01:08<00:00, 271.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.recommend_util import ndcg\n",
    "\n",
    "ndcg_values = []\n",
    "for learning_count in range(1,11):\n",
    "    ndcgs = {\n",
    "        'ndcg5':  [],\n",
    "        'ndcg10':  [],\n",
    "        'ndcg20':  [],\n",
    "        'ndcg50':  [],\n",
    "        'ndcg100':  []\n",
    "    }\n",
    "    best_model = load_model(f'output/ml-20m-model_k_{learning_count}.h5')\n",
    "\n",
    "    for userid, pos_items in tqdm(test_adventure_pos_items_dict.items()):\n",
    "       # pos_itemsをadventure_matrixの次元に変換する\n",
    "        pos_items = np.array([adventure_concat_itemid_dict[v] for v in pos_items])\n",
    "        # useridに対応するユーザベクトル(action)を得る\n",
    "        try:\n",
    "            action_userid = action_train_action_users[userid]\n",
    "        except:\n",
    "            # 推薦できないユーザの場合は無条件で0を入れる\n",
    "            ndcgs['ndcg5'].append(0)\n",
    "            ndcgs['ndcg10'].append(0)\n",
    "            ndcgs['ndcg20'].append(0)\n",
    "            ndcgs['ndcg50'].append(0)\n",
    "            ndcgs['ndcg100'].append(0)\n",
    "            continue\n",
    "\n",
    "        action_user_vector = action_ALS_user_vectors[action_userid, :]\n",
    "        # autoencoderを使ってadventureの次元に変換する\n",
    "        adventure_user_vector_action_AE = best_model.predict(action_user_vector.reshape(1, -1))\n",
    "        # adventureのitemのベクトルと掛け合わせる\n",
    "        adv_predict = np.dot(adventure_user_vector_action_AE, adventure_item_vectors.T)\n",
    "        # sum_ratingsをargsort\n",
    "        sorted_indices = np.array([v for v in np.argsort(-adv_predict)])[0]\n",
    "        ndcgs['ndcg5'].append(ndcg(sorted_indices[:5], pos_items))\n",
    "        ndcgs['ndcg10'].append(ndcg(sorted_indices[:10], pos_items))\n",
    "        ndcgs['ndcg20'].append(ndcg(sorted_indices[:20], pos_items))\n",
    "        ndcgs['ndcg50'].append(ndcg(sorted_indices[:50], pos_items))\n",
    "        ndcgs['ndcg100'].append(ndcg(sorted_indices[:100], pos_items))\n",
    "    \n",
    "    ndcg_values.append(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@5: 0.13976131508579417\n",
      "ndcg@10: 0.1865852172889774\n",
      "ndcg@20: 0.23292729816527377\n",
      "ndcg@50: 0.29863769196583057\n",
      "ndcg@100: 0.34043177618780235\n"
     ]
    }
   ],
   "source": [
    "ndcg5 = []\n",
    "ndcg10 = []\n",
    "ndcg20 = []\n",
    "ndcg50 = []\n",
    "ndcg100 = []\n",
    "for ndcgs in  ndcg_values:\n",
    "    ndcg5.append(np.mean(ndcgs['ndcg5']))\n",
    "    ndcg10.append(np.mean(ndcgs['ndcg10']))\n",
    "    ndcg20.append(np.mean(ndcgs['ndcg20']))\n",
    "    ndcg50.append(np.mean(ndcgs['ndcg50']))\n",
    "    ndcg100.append(np.mean(ndcgs['ndcg100']))\n",
    "print(\"ndcg@5: {}\".format(np.mean(ndcg5)))\n",
    "print(\"ndcg@10: {}\".format(np.mean(ndcg10)))\n",
    "print(\"ndcg@20: {}\".format(np.mean(ndcg20)))\n",
    "print(\"ndcg@50: {}\".format(np.mean(ndcg50)))\n",
    "print(\"ndcg@100: {}\".format(np.mean(ndcg100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
