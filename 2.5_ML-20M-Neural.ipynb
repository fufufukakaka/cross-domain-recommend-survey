{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "# read data\n",
    "movies = pd.read_csv(\"data/ml-20m/movies.csv\")\n",
    "ratings = pd.read_csv(\"data/ml-20m/ratings.csv\")\n",
    "\n",
    "# join\n",
    "ratings_joined = pd.merge(ratings, movies)\n",
    "\n",
    "# ratingsをsparse matrixに変換して横持ちにする\n",
    "action_adventure_ratings = ratings_joined.query(\"genres.str.contains('Action') or genres.str.contains('Adventure')\", \n",
    "                                                engine='python').reset_index(drop=True)\n",
    "# indexing ids\n",
    "# userid\n",
    "userid_unique = pd.Series(action_adventure_ratings[\"userId\"].unique())\n",
    "index_userid_dict = userid_unique.to_dict()\n",
    "# inverse\n",
    "userid_index_dict = dict(map(reversed, index_userid_dict.items()))\n",
    "\n",
    "# itemid\n",
    "itemid_unique = pd.Series(action_adventure_ratings[\"movieId\"].unique())\n",
    "index_itemid_dict = itemid_unique.to_dict()\n",
    "# inverse\n",
    "itemid_index_dict = dict(map(reversed, index_itemid_dict.items()))\n",
    "\n",
    "action_adventure_ratings[\"user_id\"] = action_adventure_ratings[\"userId\"].map(userid_index_dict)\n",
    "action_adventure_ratings[\"item_id\"] = action_adventure_ratings[\"movieId\"].map(itemid_index_dict)\n",
    "\n",
    "# reindexしたidを使って、アイテムとジャンルの対応が取れるdictを作る\n",
    "itemid_genres_dict = action_adventure_ratings[['item_id', 'genres']].set_index('item_id')['genres'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "X_train = cloudpickle.load(open(\"output/ML-20M-X_train.pkl\",\"rb\"))\n",
    "X_test = cloudpickle.load(open(\"output/ML-20M-X_test.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregateのtrainをactionとadventureに分離する\n",
    "# actionの列\n",
    "action_columns = [v for v in range(X_train.shape[1]) if 'Action' in itemid_genres_dict[v]]\n",
    "# adventureの列\n",
    "adventure_columns = [v for v in range(X_train.shape[1]) if 'Adventure' in itemid_genres_dict[v]]\n",
    "\n",
    "# 選んだカラムに応じてとってくる\n",
    "action_train = X_train[:, action_columns]\n",
    "adventure_train = X_train[:, adventure_columns]\n",
    "\n",
    "# adventureのみ、アイテムidのconcatとの対応関係が必要なので辞書として持っておく\n",
    "adventure_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'Adventure' in itemid_genres_dict[v]:\n",
    "        adventure_concat_itemid_dict[v] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アイテムidのconcatとの対応関係が必要なので辞書として持っておく\n",
    "action_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'Action' in itemid_genres_dict[v]:\n",
    "        action_concat_itemid_dict[v] = count\n",
    "        count += 1\n",
    "# inverse\n",
    "inverse_action_concat_itemid_dict = dict(map(reversed, action_concat_itemid_dict.items()))\n",
    "\n",
    "adventure_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'Adventure' in itemid_genres_dict[v]:\n",
    "        adventure_concat_itemid_dict[v] = count\n",
    "        count += 1\n",
    "# inverse\n",
    "inverse_adventure_concat_itemid_dict = dict(map(reversed, adventure_concat_itemid_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれにアクションしていないユーザを削る\n",
    "# 全ユーザと、削ったあとでの対応関係を辞書として持っておく\n",
    "action_train_selected = action_train[action_train.getnnz(1)>0]\n",
    "adventure_train_selected = adventure_train[adventure_train.getnnz(1)>0]\n",
    "\n",
    "action_train_action_users = {}\n",
    "action_users = action_train.getnnz(1)>0\n",
    "count = 0\n",
    "for i in range(action_train.shape[0]):\n",
    "    if action_users[i]:\n",
    "        action_train_action_users[i] = count\n",
    "        count += 1\n",
    "\n",
    "# inverse\n",
    "inverse_action_train_action_users = dict(map(reversed, action_train_action_users.items()))\n",
    "\n",
    "adventure_train_action_users = {}\n",
    "adventure_users = adventure_train.getnnz(1)>0\n",
    "count = 0\n",
    "for i in range(adventure_train.shape[0]):\n",
    "    if adventure_users[i]:\n",
    "        adventure_train_action_users[i] = count\n",
    "        count += 1\n",
    "\n",
    "# inverse\n",
    "inverse_adventure_train_action_users = dict(map(reversed, adventure_train_action_users.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれでALSする\n",
    "# 今回は mediateでやったときのものを使う\n",
    "action_ALS = cloudpickle.load(open('output/ML-20M-action_ALS.pkl', 'rb'))\n",
    "adventure_ALS = cloudpickle.load(open(\"output/ML-20M-adventure_ALS.pkl\",\"rb\"))\n",
    "\n",
    "action_ALS_user_vectors = action_ALS.user_factors\n",
    "adventure_ALS_user_vectors = adventure_ALS.user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138389/138389 [00:02<00:00, 50574.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# actionとadventureでoverlapしているユーザで、ベクトルの対応表を作る\n",
    "overlap_action_user_vectors = []\n",
    "overlap_adventure_user_vectors = []\n",
    "count = 0\n",
    "for u in tqdm(range(X_train.shape[0])):\n",
    "    if u in action_train_action_users and u in adventure_train_action_users:\n",
    "        overlap_action_user_vectors.append(action_ALS_user_vectors[action_train_action_users[u]].tolist())\n",
    "        overlap_adventure_user_vectors.append(adventure_ALS_user_vectors[adventure_train_action_users[u]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# AutoEncoderの学習をする\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def build_model(input_dim, output_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation='relu')(inputs)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(output_dim, activation='sigmoid')(decoded)\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae','mse'])\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 61820 samples, validate on 20607 samples\n",
      "Epoch 1/100\n",
      "61820/61820 [==============================] - 2s 30us/step - loss: 0.3746 - mae: 0.3837 - mse: 0.3746 - val_loss: 0.3447 - val_mae: 0.3566 - val_mse: 0.3447\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34468, saving model to output/ml-20m-model.h5\n",
      "Epoch 2/100\n",
      "61820/61820 [==============================] - 1s 23us/step - loss: 0.3482 - mae: 0.3567 - mse: 0.3482 - val_loss: 0.3405 - val_mae: 0.3538 - val_mse: 0.3405\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34468 to 0.34049, saving model to output/ml-20m-model.h5\n",
      "Epoch 3/100\n",
      "61820/61820 [==============================] - 1s 22us/step - loss: 0.3405 - mae: 0.3526 - mse: 0.3405 - val_loss: 0.3277 - val_mae: 0.3480 - val_mse: 0.3277\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34049 to 0.32766, saving model to output/ml-20m-model.h5\n",
      "Epoch 4/100\n",
      "61820/61820 [==============================] - 1s 22us/step - loss: 0.3197 - mae: 0.3440 - mse: 0.3197 - val_loss: 0.3051 - val_mae: 0.3376 - val_mse: 0.3051\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32766 to 0.30512, saving model to output/ml-20m-model.h5\n",
      "Epoch 5/100\n",
      "61820/61820 [==============================] - 1s 23us/step - loss: 0.3055 - mae: 0.3359 - mse: 0.3055 - val_loss: 0.2983 - val_mae: 0.3328 - val_mse: 0.2983\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30512 to 0.29834, saving model to output/ml-20m-model.h5\n",
      "Epoch 6/100\n",
      "61820/61820 [==============================] - 1s 22us/step - loss: 0.2998 - mae: 0.3316 - mse: 0.2998 - val_loss: 0.2946 - val_mae: 0.3294 - val_mse: 0.2946\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29834 to 0.29465, saving model to output/ml-20m-model.h5\n",
      "Epoch 7/100\n",
      "61820/61820 [==============================] - 1s 23us/step - loss: 0.2969 - mae: 0.3291 - mse: 0.2969 - val_loss: 0.2926 - val_mae: 0.3276 - val_mse: 0.2926\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.29465 to 0.29259, saving model to output/ml-20m-model.h5\n",
      "Epoch 8/100\n",
      "61820/61820 [==============================] - 1s 21us/step - loss: 0.2940 - mae: 0.3267 - mse: 0.2940 - val_loss: 0.2898 - val_mae: 0.3250 - val_mse: 0.2898\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.29259 to 0.28977, saving model to output/ml-20m-model.h5\n",
      "Epoch 9/100\n",
      "61820/61820 [==============================] - 1s 23us/step - loss: 0.2911 - mae: 0.3247 - mse: 0.2911 - val_loss: 0.2878 - val_mae: 0.3235 - val_mse: 0.2878\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.28977 to 0.28777, saving model to output/ml-20m-model.h5\n",
      "Epoch 10/100\n",
      "61820/61820 [==============================] - 1s 23us/step - loss: 0.2897 - mae: 0.3234 - mse: 0.2897 - val_loss: 0.2862 - val_mae: 0.3222 - val_mse: 0.2862\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.28777 to 0.28616, saving model to output/ml-20m-model.h5\n",
      "Epoch 11/100\n",
      "61820/61820 [==============================] - 1s 21us/step - loss: 0.2879 - mae: 0.3219 - mse: 0.2879 - val_loss: 0.2846 - val_mae: 0.3209 - val_mse: 0.2846\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.28616 to 0.28459, saving model to output/ml-20m-model.h5\n",
      "Epoch 12/100\n",
      "61820/61820 [==============================] - 1s 22us/step - loss: 0.2865 - mae: 0.3207 - mse: 0.2865 - val_loss: 0.2833 - val_mae: 0.3200 - val_mse: 0.2833\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.28459 to 0.28329, saving model to output/ml-20m-model.h5\n",
      "Epoch 13/100\n",
      "61820/61820 [==============================] - 1s 22us/step - loss: 0.2851 - mae: 0.3197 - mse: 0.2851 - val_loss: 0.2828 - val_mae: 0.3194 - val_mse: 0.2828\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.28329 to 0.28280, saving model to output/ml-20m-model.h5\n",
      "Epoch 14/100\n",
      "61820/61820 [==============================] - 1s 22us/step - loss: 0.2843 - mae: 0.3189 - mse: 0.2843 - val_loss: 0.2822 - val_mae: 0.3188 - val_mse: 0.2822\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.28280 to 0.28221, saving model to output/ml-20m-model.h5\n",
      "Epoch 15/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2835 - mae: 0.3182 - mse: 0.2835 - val_loss: 0.2813 - val_mae: 0.3180 - val_mse: 0.2813\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.28221 to 0.28132, saving model to output/ml-20m-model.h5\n",
      "Epoch 16/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2827 - mae: 0.3173 - mse: 0.2827 - val_loss: 0.2805 - val_mae: 0.3171 - val_mse: 0.2805\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.28132 to 0.28049, saving model to output/ml-20m-model.h5\n",
      "Epoch 17/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2820 - mae: 0.3166 - mse: 0.2820 - val_loss: 0.2797 - val_mae: 0.3163 - val_mse: 0.2797\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.28049 to 0.27972, saving model to output/ml-20m-model.h5\n",
      "Epoch 18/100\n",
      "61820/61820 [==============================] - 1s 18us/step - loss: 0.2812 - mae: 0.3159 - mse: 0.2812 - val_loss: 0.2794 - val_mae: 0.3160 - val_mse: 0.2794\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.27972 to 0.27943, saving model to output/ml-20m-model.h5\n",
      "Epoch 19/100\n",
      "61820/61820 [==============================] - 1s 20us/step - loss: 0.2806 - mae: 0.3154 - mse: 0.2806 - val_loss: 0.2787 - val_mae: 0.3151 - val_mse: 0.2787\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.27943 to 0.27872, saving model to output/ml-20m-model.h5\n",
      "Epoch 20/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2799 - mae: 0.3147 - mse: 0.2799 - val_loss: 0.2780 - val_mae: 0.3148 - val_mse: 0.2780\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.27872 to 0.27799, saving model to output/ml-20m-model.h5\n",
      "Epoch 21/100\n",
      "61820/61820 [==============================] - 1s 20us/step - loss: 0.2792 - mae: 0.3142 - mse: 0.2792 - val_loss: 0.2777 - val_mae: 0.3144 - val_mse: 0.2777\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.27799 to 0.27772, saving model to output/ml-20m-model.h5\n",
      "Epoch 22/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2789 - mae: 0.3137 - mse: 0.2789 - val_loss: 0.2775 - val_mae: 0.3142 - val_mse: 0.2775\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.27772 to 0.27751, saving model to output/ml-20m-model.h5\n",
      "Epoch 23/100\n",
      "61820/61820 [==============================] - 1s 18us/step - loss: 0.2784 - mae: 0.3132 - mse: 0.2784 - val_loss: 0.2769 - val_mae: 0.3134 - val_mse: 0.2769\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.27751 to 0.27691, saving model to output/ml-20m-model.h5\n",
      "Epoch 24/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2778 - mae: 0.3127 - mse: 0.2778 - val_loss: 0.2761 - val_mae: 0.3125 - val_mse: 0.2761\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.27691 to 0.27615, saving model to output/ml-20m-model.h5\n",
      "Epoch 25/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2774 - mae: 0.3123 - mse: 0.2774 - val_loss: 0.2761 - val_mae: 0.3126 - val_mse: 0.2761\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.27615 to 0.27608, saving model to output/ml-20m-model.h5\n",
      "Epoch 26/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2771 - mae: 0.3120 - mse: 0.2771 - val_loss: 0.2759 - val_mae: 0.3121 - val_mse: 0.2759\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.27608 to 0.27586, saving model to output/ml-20m-model.h5\n",
      "Epoch 27/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2767 - mae: 0.3116 - mse: 0.2767 - val_loss: 0.2756 - val_mae: 0.3121 - val_mse: 0.2756\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.27586 to 0.27561, saving model to output/ml-20m-model.h5\n",
      "Epoch 28/100\n",
      "61820/61820 [==============================] - 1s 18us/step - loss: 0.2764 - mae: 0.3113 - mse: 0.2764 - val_loss: 0.2752 - val_mae: 0.3117 - val_mse: 0.2752\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.27561 to 0.27524, saving model to output/ml-20m-model.h5\n",
      "Epoch 29/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2761 - mae: 0.3110 - mse: 0.2761 - val_loss: 0.2750 - val_mae: 0.3114 - val_mse: 0.2750\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.27524 to 0.27502, saving model to output/ml-20m-model.h5\n",
      "Epoch 30/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2758 - mae: 0.3106 - mse: 0.2758 - val_loss: 0.2749 - val_mae: 0.3112 - val_mse: 0.2749\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.27502 to 0.27488, saving model to output/ml-20m-model.h5\n",
      "Epoch 31/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2756 - mae: 0.3104 - mse: 0.2756 - val_loss: 0.2745 - val_mae: 0.3107 - val_mse: 0.2745\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.27488 to 0.27454, saving model to output/ml-20m-model.h5\n",
      "Epoch 32/100\n",
      "61820/61820 [==============================] - 1s 18us/step - loss: 0.2753 - mae: 0.3101 - mse: 0.2753 - val_loss: 0.2746 - val_mae: 0.3106 - val_mse: 0.2746\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.27454\n",
      "Epoch 33/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2750 - mae: 0.3098 - mse: 0.2750 - val_loss: 0.2745 - val_mae: 0.3104 - val_mse: 0.2745\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.27454 to 0.27447, saving model to output/ml-20m-model.h5\n",
      "Epoch 34/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2749 - mae: 0.3096 - mse: 0.2749 - val_loss: 0.2745 - val_mae: 0.3102 - val_mse: 0.2745\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.27447 to 0.27446, saving model to output/ml-20m-model.h5\n",
      "Epoch 35/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2747 - mae: 0.3094 - mse: 0.2747 - val_loss: 0.2740 - val_mae: 0.3101 - val_mse: 0.2740\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.27446 to 0.27405, saving model to output/ml-20m-model.h5\n",
      "Epoch 36/100\n",
      "61820/61820 [==============================] - 1s 18us/step - loss: 0.2745 - mae: 0.3092 - mse: 0.2745 - val_loss: 0.2739 - val_mae: 0.3098 - val_mse: 0.2739\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.27405 to 0.27387, saving model to output/ml-20m-model.h5\n",
      "Epoch 37/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2743 - mae: 0.3090 - mse: 0.2743 - val_loss: 0.2738 - val_mae: 0.3098 - val_mse: 0.2738\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.27387 to 0.27379, saving model to output/ml-20m-model.h5\n",
      "Epoch 38/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2741 - mae: 0.3088 - mse: 0.2741 - val_loss: 0.2737 - val_mae: 0.3095 - val_mse: 0.2737\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.27379 to 0.27372, saving model to output/ml-20m-model.h5\n",
      "Epoch 39/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2740 - mae: 0.3086 - mse: 0.2740 - val_loss: 0.2739 - val_mae: 0.3098 - val_mse: 0.2739\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.27372\n",
      "Epoch 40/100\n",
      "61820/61820 [==============================] - 1s 19us/step - loss: 0.2737 - mae: 0.3083 - mse: 0.2737 - val_loss: 0.2734 - val_mae: 0.3091 - val_mse: 0.2734\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.27372 to 0.27340, saving model to output/ml-20m-model.h5\n",
      "Epoch 41/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2735 - mae: 0.3081 - mse: 0.2735 - val_loss: 0.2737 - val_mae: 0.3093 - val_mse: 0.2737\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.27340\n",
      "Epoch 42/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2734 - mae: 0.3080 - mse: 0.2734 - val_loss: 0.2732 - val_mae: 0.3088 - val_mse: 0.2732\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.27340 to 0.27319, saving model to output/ml-20m-model.h5\n",
      "Epoch 43/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2733 - mae: 0.3078 - mse: 0.2733 - val_loss: 0.2732 - val_mae: 0.3088 - val_mse: 0.2732\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.27319 to 0.27316, saving model to output/ml-20m-model.h5\n",
      "Epoch 44/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2731 - mae: 0.3076 - mse: 0.2731 - val_loss: 0.2732 - val_mae: 0.3087 - val_mse: 0.2732\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.27316\n",
      "Epoch 45/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2730 - mae: 0.3075 - mse: 0.2730 - val_loss: 0.2731 - val_mae: 0.3087 - val_mse: 0.2731\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.27316 to 0.27313, saving model to output/ml-20m-model.h5\n",
      "Epoch 46/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2729 - mae: 0.3074 - mse: 0.2729 - val_loss: 0.2730 - val_mae: 0.3085 - val_mse: 0.2730\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.27313 to 0.27298, saving model to output/ml-20m-model.h5\n",
      "Epoch 47/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2727 - mae: 0.3072 - mse: 0.2727 - val_loss: 0.2728 - val_mae: 0.3083 - val_mse: 0.2728\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.27298 to 0.27284, saving model to output/ml-20m-model.h5\n",
      "Epoch 48/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2726 - mae: 0.3071 - mse: 0.2726 - val_loss: 0.2725 - val_mae: 0.3078 - val_mse: 0.2725\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.27284 to 0.27246, saving model to output/ml-20m-model.h5\n",
      "Epoch 49/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2724 - mae: 0.3069 - mse: 0.2724 - val_loss: 0.2725 - val_mae: 0.3079 - val_mse: 0.2725\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.27246\n",
      "Epoch 50/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2724 - mae: 0.3069 - mse: 0.2724 - val_loss: 0.2726 - val_mae: 0.3080 - val_mse: 0.2726\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.27246\n",
      "Epoch 51/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2723 - mae: 0.3067 - mse: 0.2723 - val_loss: 0.2724 - val_mae: 0.3078 - val_mse: 0.2724\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.27246 to 0.27235, saving model to output/ml-20m-model.h5\n",
      "Epoch 52/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2721 - mae: 0.3066 - mse: 0.2721 - val_loss: 0.2726 - val_mae: 0.3078 - val_mse: 0.2726\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.27235\n",
      "Epoch 53/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2721 - mae: 0.3066 - mse: 0.2721 - val_loss: 0.2724 - val_mae: 0.3076 - val_mse: 0.2724\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.27235\n",
      "Epoch 54/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2719 - mae: 0.3064 - mse: 0.2719 - val_loss: 0.2725 - val_mae: 0.3078 - val_mse: 0.2725\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.27235\n",
      "Epoch 55/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2719 - mae: 0.3063 - mse: 0.2719 - val_loss: 0.2724 - val_mae: 0.3078 - val_mse: 0.2724\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.27235\n",
      "Epoch 56/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2718 - mae: 0.3062 - mse: 0.2718 - val_loss: 0.2722 - val_mae: 0.3074 - val_mse: 0.2722\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.27235 to 0.27221, saving model to output/ml-20m-model.h5\n",
      "Epoch 57/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2717 - mae: 0.3062 - mse: 0.2717 - val_loss: 0.2722 - val_mae: 0.3073 - val_mse: 0.2722\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.27221\n",
      "Epoch 58/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2716 - mae: 0.3060 - mse: 0.2716 - val_loss: 0.2723 - val_mae: 0.3075 - val_mse: 0.2723\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.27221\n",
      "Epoch 59/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2715 - mae: 0.3059 - mse: 0.2715 - val_loss: 0.2721 - val_mae: 0.3072 - val_mse: 0.2721\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.27221 to 0.27208, saving model to output/ml-20m-model.h5\n",
      "Epoch 60/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2714 - mae: 0.3058 - mse: 0.2714 - val_loss: 0.2721 - val_mae: 0.3071 - val_mse: 0.2721\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.27208\n",
      "Epoch 61/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2714 - mae: 0.3058 - mse: 0.2714 - val_loss: 0.2720 - val_mae: 0.3073 - val_mse: 0.2720\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.27208 to 0.27200, saving model to output/ml-20m-model.h5\n",
      "Epoch 62/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2713 - mae: 0.3058 - mse: 0.2713 - val_loss: 0.2721 - val_mae: 0.3072 - val_mse: 0.2721\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.27200\n",
      "Epoch 63/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2713 - mae: 0.3057 - mse: 0.2713 - val_loss: 0.2718 - val_mae: 0.3068 - val_mse: 0.2718\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.27200 to 0.27177, saving model to output/ml-20m-model.h5\n",
      "Epoch 64/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2711 - mae: 0.3055 - mse: 0.2711 - val_loss: 0.2719 - val_mae: 0.3068 - val_mse: 0.2719\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.27177\n",
      "Epoch 65/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2711 - mae: 0.3055 - mse: 0.2711 - val_loss: 0.2718 - val_mae: 0.3068 - val_mse: 0.2718\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.27177\n",
      "Epoch 66/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2709 - mae: 0.3054 - mse: 0.2709 - val_loss: 0.2719 - val_mae: 0.3067 - val_mse: 0.2719\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.27177\n",
      "Epoch 67/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2710 - mae: 0.3054 - mse: 0.2710 - val_loss: 0.2718 - val_mae: 0.3065 - val_mse: 0.2718\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.27177 to 0.27177, saving model to output/ml-20m-model.h5\n",
      "Epoch 68/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2708 - mae: 0.3053 - mse: 0.2708 - val_loss: 0.2718 - val_mae: 0.3068 - val_mse: 0.2718\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.27177\n",
      "Epoch 69/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2707 - mae: 0.3051 - mse: 0.2707 - val_loss: 0.2717 - val_mae: 0.3066 - val_mse: 0.2717\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.27177 to 0.27170, saving model to output/ml-20m-model.h5\n",
      "Epoch 70/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2707 - mae: 0.3051 - mse: 0.2707 - val_loss: 0.2717 - val_mae: 0.3065 - val_mse: 0.2717\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.27170 to 0.27167, saving model to output/ml-20m-model.h5\n",
      "Epoch 71/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2707 - mae: 0.3051 - mse: 0.2707 - val_loss: 0.2716 - val_mae: 0.3064 - val_mse: 0.2716\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.27167 to 0.27161, saving model to output/ml-20m-model.h5\n",
      "Epoch 72/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2706 - mae: 0.3050 - mse: 0.2706 - val_loss: 0.2716 - val_mae: 0.3065 - val_mse: 0.2716\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.27161 to 0.27155, saving model to output/ml-20m-model.h5\n",
      "Epoch 73/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2705 - mae: 0.3049 - mse: 0.2705 - val_loss: 0.2715 - val_mae: 0.3061 - val_mse: 0.2715\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.27155 to 0.27145, saving model to output/ml-20m-model.h5\n",
      "Epoch 74/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2704 - mae: 0.3048 - mse: 0.2704 - val_loss: 0.2715 - val_mae: 0.3064 - val_mse: 0.2715\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.27145\n",
      "Epoch 75/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2704 - mae: 0.3048 - mse: 0.2704 - val_loss: 0.2714 - val_mae: 0.3062 - val_mse: 0.2714\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.27145 to 0.27142, saving model to output/ml-20m-model.h5\n",
      "Epoch 76/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2704 - mae: 0.3048 - mse: 0.2704 - val_loss: 0.2715 - val_mae: 0.3061 - val_mse: 0.2715\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.27142\n",
      "Epoch 77/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2704 - mae: 0.3047 - mse: 0.2704 - val_loss: 0.2716 - val_mae: 0.3065 - val_mse: 0.2716\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.27142\n",
      "Epoch 78/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2703 - mae: 0.3047 - mse: 0.2703 - val_loss: 0.2716 - val_mae: 0.3063 - val_mse: 0.2716\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.27142\n",
      "Epoch 79/100\n",
      "61820/61820 [==============================] - 1s 17us/step - loss: 0.2703 - mae: 0.3047 - mse: 0.2703 - val_loss: 0.2714 - val_mae: 0.3061 - val_mse: 0.2714\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.27142\n",
      "Epoch 80/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2702 - mae: 0.3046 - mse: 0.2702 - val_loss: 0.2714 - val_mae: 0.3062 - val_mse: 0.2714\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.27142\n",
      "Epoch 81/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2701 - mae: 0.3045 - mse: 0.2701 - val_loss: 0.2714 - val_mae: 0.3061 - val_mse: 0.2714\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.27142 to 0.27139, saving model to output/ml-20m-model.h5\n",
      "Epoch 82/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2700 - mae: 0.3044 - mse: 0.2700 - val_loss: 0.2718 - val_mae: 0.3067 - val_mse: 0.2718\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.27139\n",
      "Epoch 83/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2700 - mae: 0.3044 - mse: 0.2700 - val_loss: 0.2717 - val_mae: 0.3065 - val_mse: 0.2717\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.27139\n",
      "Epoch 84/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 0.2699 - mae: 0.3043 - mse: 0.2699 - val_loss: 0.2716 - val_mae: 0.3064 - val_mse: 0.2716\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.27139\n",
      "Epoch 85/100\n",
      "61820/61820 [==============================] - 1s 17us/step - loss: 0.2699 - mae: 0.3043 - mse: 0.2699 - val_loss: 0.2718 - val_mae: 0.3068 - val_mse: 0.2718\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.27139\n",
      "Epoch 86/100\n",
      "61820/61820 [==============================] - 1s 17us/step - loss: 0.2699 - mae: 0.3042 - mse: 0.2699 - val_loss: 0.2715 - val_mae: 0.3062 - val_mse: 0.2715\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.27139\n",
      "Epoch 87/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2698 - mae: 0.3042 - mse: 0.2698 - val_loss: 0.2716 - val_mae: 0.3061 - val_mse: 0.2716\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.27139\n",
      "Epoch 88/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2698 - mae: 0.3041 - mse: 0.2698 - val_loss: 0.2713 - val_mae: 0.3059 - val_mse: 0.2713\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.27139 to 0.27132, saving model to output/ml-20m-model.h5\n",
      "Epoch 89/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2697 - mae: 0.3040 - mse: 0.2697 - val_loss: 0.2714 - val_mae: 0.3061 - val_mse: 0.2714\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.27132\n",
      "Epoch 90/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2696 - mae: 0.3040 - mse: 0.2696 - val_loss: 0.2713 - val_mae: 0.3059 - val_mse: 0.2713\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.27132\n",
      "Epoch 91/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2696 - mae: 0.3039 - mse: 0.2696 - val_loss: 0.2712 - val_mae: 0.3057 - val_mse: 0.2712\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.27132 to 0.27117, saving model to output/ml-20m-model.h5\n",
      "Epoch 92/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2696 - mae: 0.3039 - mse: 0.2696 - val_loss: 0.2712 - val_mae: 0.3057 - val_mse: 0.2712\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.27117\n",
      "Epoch 93/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2695 - mae: 0.3039 - mse: 0.2695 - val_loss: 0.2714 - val_mae: 0.3058 - val_mse: 0.2714\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.27117\n",
      "Epoch 94/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2695 - mae: 0.3038 - mse: 0.2695 - val_loss: 0.2710 - val_mae: 0.3056 - val_mse: 0.2710\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.27117 to 0.27103, saving model to output/ml-20m-model.h5\n",
      "Epoch 95/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2694 - mae: 0.3038 - mse: 0.2694 - val_loss: 0.2715 - val_mae: 0.3060 - val_mse: 0.2715\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.27103\n",
      "Epoch 96/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2694 - mae: 0.3037 - mse: 0.2694 - val_loss: 0.2714 - val_mae: 0.3057 - val_mse: 0.2714\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.27103\n",
      "Epoch 97/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2694 - mae: 0.3037 - mse: 0.2694 - val_loss: 0.2712 - val_mae: 0.3056 - val_mse: 0.2712\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.27103\n",
      "Epoch 98/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2694 - mae: 0.3037 - mse: 0.2694 - val_loss: 0.2714 - val_mae: 0.3058 - val_mse: 0.2714\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.27103\n",
      "Epoch 99/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2692 - mae: 0.3036 - mse: 0.2692 - val_loss: 0.2711 - val_mae: 0.3054 - val_mse: 0.2711\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.27103\n",
      "Epoch 100/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 0.2692 - mae: 0.3036 - mse: 0.2692 - val_loss: 0.2711 - val_mae: 0.3055 - val_mse: 0.2711\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.27103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7feb6a59ff28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_vector, X_test_vector, y_train_vector, y_test_vector = train_test_split(overlap_action_user_vectors, overlap_adventure_user_vectors, random_state=42)\n",
    "X_train_vector, X_val_vector, y_train_vector, y_val_vector = train_test_split(X_train_vector, y_train_vector, random_state=42)\n",
    "\n",
    "epoch_size = 100\n",
    "batch_size = 256\n",
    "mcheck = ModelCheckpoint(\n",
    "    'output/ml-20m-model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "es_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "model = build_model(np.array(X_train_vector).shape[1], np.array(y_train_vector).shape[1])\n",
    "model.fit(\n",
    "    np.array(X_train_vector),\n",
    "    np.array(y_train_vector),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_size,\n",
    "    validation_data=(\n",
    "        np.array(X_val_vector),\n",
    "        np.array(y_val_vector)),\n",
    "    callbacks=[\n",
    "        mcheck,\n",
    "        es_cb],\n",
    "    shuffle=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.5244525355572482\n"
     ]
    }
   ],
   "source": [
    "# テストデータに対するRMSE計算\n",
    "from sklearn.metrics import mean_squared_error\n",
    "best_model = load_model('output/ml-20m-model.h5')\n",
    "y_pred = best_model.predict(np.array(X_test_vector))\n",
    "rmse_ = np.sqrt(mean_squared_error(y_pred, np.array(y_test_vector)))\n",
    "print('rmse: {}'.format(rmse_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaeとで、rmseを比較し、良い方を選ぶ\n",
    "# ref. https://keras.io/examples/variational_autoencoder/\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "class VAE():\n",
    "    def __init__(self, input_dim, intermediate_dim, latent_dim, original_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.original_dim = original_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.z_mean = None\n",
    "        self.z_log_var = None\n",
    "\n",
    "\n",
    "    # reparameterization trick\n",
    "    # instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "    # z = z_mean + sqrt(var) * epsilon\n",
    "    def sampling(self, args):\n",
    "        \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "        # Arguments\n",
    "            args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "        # Returns\n",
    "            z (tensor): sampled latent vector\n",
    "        \"\"\"\n",
    "\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        # by default, random_normal has mean = 0 and std = 1.0\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "    def vae_loss(self, y_true, y_pred):\n",
    "        reconstruction_loss = mse(y_true, y_pred)\n",
    "        reconstruction_loss *= self.original_dim\n",
    "        kl_loss = 1 + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        return vae_loss\n",
    "\n",
    "\n",
    "    def build_vae(self):\n",
    "        # VAE model = encoder + decoder\n",
    "        # build encoder model\n",
    "        inputs = Input(shape=(self.input_dim,), name='encoder_input')\n",
    "        x = Dense(128, activation='relu')(inputs)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        self.z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        self.z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "\n",
    "        # use reparameterization trick to push the sampling out as input\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(self.sampling, output_shape=(self.latent_dim,), name='z')([self.z_mean, self.z_log_var])\n",
    "\n",
    "        # instantiate encoder model\n",
    "        encoder = Model(inputs, [self.z_mean, self.z_log_var, z], name='encoder')\n",
    "\n",
    "        # build decoder model\n",
    "        latent_inputs = Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        x = Dense(32, activation='relu')(latent_inputs)\n",
    "        x = Dense(64, activation='relu')(latent_inputs)\n",
    "        x = Dense(128, activation='relu')(latent_inputs)\n",
    "        decoder_outputs = Dense(self.original_dim, activation='sigmoid')(x)\n",
    "\n",
    "        # instantiate decoder model\n",
    "        decoder = Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "        # instantiate VAE model\n",
    "        outputs = decoder(encoder(inputs)[2])\n",
    "        vae = Model(inputs, outputs, name='vae')\n",
    "        vae.compile(optimizer='adam', loss=self.vae_loss)\n",
    "        return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(np.array(X_train_vector).shape[1], 256, 2, np.array(y_train_vector).shape[1])\n",
    "model = vae.build_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 23396     \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 100)               13284     \n",
      "=================================================================\n",
      "Total params: 36,680\n",
      "Trainable params: 36,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61820 samples, validate on 20607 samples\n",
      "Epoch 1/100\n",
      "61820/61820 [==============================] - 1s 23us/step - loss: 40.9104 - val_loss: 34.9151\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 34.91512, saving model to output/ml-20m-vae.h5\n",
      "Epoch 2/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 34.6056 - val_loss: 33.6632\n",
      "\n",
      "Epoch 00002: val_loss improved from 34.91512 to 33.66316, saving model to output/ml-20m-vae.h5\n",
      "Epoch 3/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 33.7799 - val_loss: 33.1645\n",
      "\n",
      "Epoch 00003: val_loss improved from 33.66316 to 33.16448, saving model to output/ml-20m-vae.h5\n",
      "Epoch 4/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 33.5114 - val_loss: 33.0218\n",
      "\n",
      "Epoch 00004: val_loss improved from 33.16448 to 33.02184, saving model to output/ml-20m-vae.h5\n",
      "Epoch 5/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 33.4078 - val_loss: 32.9593\n",
      "\n",
      "Epoch 00005: val_loss improved from 33.02184 to 32.95927, saving model to output/ml-20m-vae.h5\n",
      "Epoch 6/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 33.3494 - val_loss: 32.9113\n",
      "\n",
      "Epoch 00006: val_loss improved from 32.95927 to 32.91134, saving model to output/ml-20m-vae.h5\n",
      "Epoch 7/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 33.2787 - val_loss: 32.8587\n",
      "\n",
      "Epoch 00007: val_loss improved from 32.91134 to 32.85866, saving model to output/ml-20m-vae.h5\n",
      "Epoch 8/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 33.2342 - val_loss: 32.7911\n",
      "\n",
      "Epoch 00008: val_loss improved from 32.85866 to 32.79109, saving model to output/ml-20m-vae.h5\n",
      "Epoch 9/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 33.1369 - val_loss: 32.7456\n",
      "\n",
      "Epoch 00009: val_loss improved from 32.79109 to 32.74560, saving model to output/ml-20m-vae.h5\n",
      "Epoch 10/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 33.1025 - val_loss: 32.7004\n",
      "\n",
      "Epoch 00010: val_loss improved from 32.74560 to 32.70038, saving model to output/ml-20m-vae.h5\n",
      "Epoch 11/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 33.0650 - val_loss: 32.6377\n",
      "\n",
      "Epoch 00011: val_loss improved from 32.70038 to 32.63772, saving model to output/ml-20m-vae.h5\n",
      "Epoch 12/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 33.0330 - val_loss: 32.6256\n",
      "\n",
      "Epoch 00012: val_loss improved from 32.63772 to 32.62558, saving model to output/ml-20m-vae.h5\n",
      "Epoch 13/100\n",
      "61820/61820 [==============================] - 1s 16us/step - loss: 33.0133 - val_loss: 32.5893\n",
      "\n",
      "Epoch 00013: val_loss improved from 32.62558 to 32.58931, saving model to output/ml-20m-vae.h5\n",
      "Epoch 14/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.9945 - val_loss: 32.5936\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 32.58931\n",
      "Epoch 15/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.9798 - val_loss: 32.5724\n",
      "\n",
      "Epoch 00015: val_loss improved from 32.58931 to 32.57240, saving model to output/ml-20m-vae.h5\n",
      "Epoch 16/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.9695 - val_loss: 32.5864\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 32.57240\n",
      "Epoch 17/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.9457 - val_loss: 32.5639\n",
      "\n",
      "Epoch 00017: val_loss improved from 32.57240 to 32.56390, saving model to output/ml-20m-vae.h5\n",
      "Epoch 18/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.9489 - val_loss: 32.5684\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 32.56390\n",
      "Epoch 19/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.9347 - val_loss: 32.5420\n",
      "\n",
      "Epoch 00019: val_loss improved from 32.56390 to 32.54199, saving model to output/ml-20m-vae.h5\n",
      "Epoch 20/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.9264 - val_loss: 32.5413\n",
      "\n",
      "Epoch 00020: val_loss improved from 32.54199 to 32.54132, saving model to output/ml-20m-vae.h5\n",
      "Epoch 21/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.9090 - val_loss: 32.5366\n",
      "\n",
      "Epoch 00021: val_loss improved from 32.54132 to 32.53663, saving model to output/ml-20m-vae.h5\n",
      "Epoch 22/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.9034 - val_loss: 32.5325\n",
      "\n",
      "Epoch 00022: val_loss improved from 32.53663 to 32.53247, saving model to output/ml-20m-vae.h5\n",
      "Epoch 23/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8868 - val_loss: 32.5257\n",
      "\n",
      "Epoch 00023: val_loss improved from 32.53247 to 32.52569, saving model to output/ml-20m-vae.h5\n",
      "Epoch 24/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8747 - val_loss: 32.5171\n",
      "\n",
      "Epoch 00024: val_loss improved from 32.52569 to 32.51707, saving model to output/ml-20m-vae.h5\n",
      "Epoch 25/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8775 - val_loss: 32.5061\n",
      "\n",
      "Epoch 00025: val_loss improved from 32.51707 to 32.50607, saving model to output/ml-20m-vae.h5\n",
      "Epoch 26/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8740 - val_loss: 32.4944\n",
      "\n",
      "Epoch 00026: val_loss improved from 32.50607 to 32.49437, saving model to output/ml-20m-vae.h5\n",
      "Epoch 27/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8506 - val_loss: 32.4950\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 32.49437\n",
      "Epoch 28/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8599 - val_loss: 32.5070\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 32.49437\n",
      "Epoch 29/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8481 - val_loss: 32.4772\n",
      "\n",
      "Epoch 00029: val_loss improved from 32.49437 to 32.47723, saving model to output/ml-20m-vae.h5\n",
      "Epoch 30/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8314 - val_loss: 32.4938\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 32.47723\n",
      "Epoch 31/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8385 - val_loss: 32.5009\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 32.47723\n",
      "Epoch 32/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8220 - val_loss: 32.4691\n",
      "\n",
      "Epoch 00032: val_loss improved from 32.47723 to 32.46907, saving model to output/ml-20m-vae.h5\n",
      "Epoch 33/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8148 - val_loss: 32.4520\n",
      "\n",
      "Epoch 00033: val_loss improved from 32.46907 to 32.45199, saving model to output/ml-20m-vae.h5\n",
      "Epoch 34/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8093 - val_loss: 32.4567\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 32.45199\n",
      "Epoch 35/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7988 - val_loss: 32.4628\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 32.45199\n",
      "Epoch 36/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.8008 - val_loss: 32.4632\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 32.45199\n",
      "Epoch 37/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7903 - val_loss: 32.4412\n",
      "\n",
      "Epoch 00037: val_loss improved from 32.45199 to 32.44116, saving model to output/ml-20m-vae.h5\n",
      "Epoch 38/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7902 - val_loss: 32.4536\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 32.44116\n",
      "Epoch 39/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7913 - val_loss: 32.4543\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 32.44116\n",
      "Epoch 40/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7754 - val_loss: 32.4563\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 32.44116\n",
      "Epoch 41/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7735 - val_loss: 32.4644\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 32.44116\n",
      "Epoch 42/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7737 - val_loss: 32.4270\n",
      "\n",
      "Epoch 00042: val_loss improved from 32.44116 to 32.42704, saving model to output/ml-20m-vae.h5\n",
      "Epoch 43/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7580 - val_loss: 32.4338\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 32.42704\n",
      "Epoch 44/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7645 - val_loss: 32.4288\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 32.42704\n",
      "Epoch 45/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7594 - val_loss: 32.4338\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 32.42704\n",
      "Epoch 46/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7520 - val_loss: 32.4245\n",
      "\n",
      "Epoch 00046: val_loss improved from 32.42704 to 32.42446, saving model to output/ml-20m-vae.h5\n",
      "Epoch 47/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7488 - val_loss: 32.4375\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 32.42446\n",
      "Epoch 48/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7410 - val_loss: 32.4368\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 32.42446\n",
      "Epoch 49/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7439 - val_loss: 32.4131\n",
      "\n",
      "Epoch 00049: val_loss improved from 32.42446 to 32.41309, saving model to output/ml-20m-vae.h5\n",
      "Epoch 50/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7416 - val_loss: 32.4370\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 32.41309\n",
      "Epoch 51/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7351 - val_loss: 32.4268\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 32.41309\n",
      "Epoch 52/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7331 - val_loss: 32.4233\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 32.41309\n",
      "Epoch 53/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7317 - val_loss: 32.4225\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 32.41309\n",
      "Epoch 54/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7225 - val_loss: 32.4269\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 32.41309\n",
      "Epoch 55/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7364 - val_loss: 32.3932\n",
      "\n",
      "Epoch 00055: val_loss improved from 32.41309 to 32.39324, saving model to output/ml-20m-vae.h5\n",
      "Epoch 56/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7152 - val_loss: 32.4140\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 32.39324\n",
      "Epoch 57/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7289 - val_loss: 32.4091\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 32.39324\n",
      "Epoch 58/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7091 - val_loss: 32.4085\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 32.39324\n",
      "Epoch 59/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7145 - val_loss: 32.3994\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 32.39324\n",
      "Epoch 60/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6962 - val_loss: 32.3914\n",
      "\n",
      "Epoch 00060: val_loss improved from 32.39324 to 32.39139, saving model to output/ml-20m-vae.h5\n",
      "Epoch 61/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7068 - val_loss: 32.3883\n",
      "\n",
      "Epoch 00061: val_loss improved from 32.39139 to 32.38829, saving model to output/ml-20m-vae.h5\n",
      "Epoch 62/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.7022 - val_loss: 32.3900\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 32.38829\n",
      "Epoch 63/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6962 - val_loss: 32.4008\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 32.38829\n",
      "Epoch 64/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6960 - val_loss: 32.3964\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 32.38829\n",
      "Epoch 65/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6881 - val_loss: 32.4006\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 32.38829\n",
      "Epoch 66/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6789 - val_loss: 32.4076\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 32.38829\n",
      "Epoch 67/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6912 - val_loss: 32.3818\n",
      "\n",
      "Epoch 00067: val_loss improved from 32.38829 to 32.38176, saving model to output/ml-20m-vae.h5\n",
      "Epoch 68/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6765 - val_loss: 32.3923\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 32.38176\n",
      "Epoch 69/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6859 - val_loss: 32.3924\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 32.38176\n",
      "Epoch 70/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6842 - val_loss: 32.4039\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 32.38176\n",
      "Epoch 71/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6697 - val_loss: 32.3974\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 32.38176\n",
      "Epoch 72/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6653 - val_loss: 32.4079\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 32.38176\n",
      "Epoch 73/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6620 - val_loss: 32.3714\n",
      "\n",
      "Epoch 00073: val_loss improved from 32.38176 to 32.37140, saving model to output/ml-20m-vae.h5\n",
      "Epoch 74/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6547 - val_loss: 32.3763\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 32.37140\n",
      "Epoch 75/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6592 - val_loss: 32.3708\n",
      "\n",
      "Epoch 00075: val_loss improved from 32.37140 to 32.37080, saving model to output/ml-20m-vae.h5\n",
      "Epoch 76/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6566 - val_loss: 32.3658\n",
      "\n",
      "Epoch 00076: val_loss improved from 32.37080 to 32.36575, saving model to output/ml-20m-vae.h5\n",
      "Epoch 77/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6596 - val_loss: 32.3906\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 32.36575\n",
      "Epoch 78/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6562 - val_loss: 32.3761\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 32.36575\n",
      "Epoch 79/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6455 - val_loss: 32.3693\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 32.36575\n",
      "Epoch 80/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6454 - val_loss: 32.3486\n",
      "\n",
      "Epoch 00080: val_loss improved from 32.36575 to 32.34861, saving model to output/ml-20m-vae.h5\n",
      "Epoch 81/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6490 - val_loss: 32.3858\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 32.34861\n",
      "Epoch 82/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6397 - val_loss: 32.3629\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 32.34861\n",
      "Epoch 83/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6417 - val_loss: 32.3638\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 32.34861\n",
      "Epoch 84/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6308 - val_loss: 32.3721\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 32.34861\n",
      "Epoch 85/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6354 - val_loss: 32.3571\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 32.34861\n",
      "Epoch 86/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6379 - val_loss: 32.3697\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 32.34861\n",
      "Epoch 87/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6380 - val_loss: 32.3647\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 32.34861\n",
      "Epoch 88/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6307 - val_loss: 32.3628\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 32.34861\n",
      "Epoch 89/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6356 - val_loss: 32.3577\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 32.34861\n",
      "Epoch 90/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6325 - val_loss: 32.3470\n",
      "\n",
      "Epoch 00090: val_loss improved from 32.34861 to 32.34698, saving model to output/ml-20m-vae.h5\n",
      "Epoch 91/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6256 - val_loss: 32.3666\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 32.34698\n",
      "Epoch 92/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6307 - val_loss: 32.3661\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 32.34698\n",
      "Epoch 93/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6203 - val_loss: 32.3381\n",
      "\n",
      "Epoch 00093: val_loss improved from 32.34698 to 32.33806, saving model to output/ml-20m-vae.h5\n",
      "Epoch 94/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6185 - val_loss: 32.3666\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 32.33806\n",
      "Epoch 95/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6280 - val_loss: 32.3833\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 32.33806\n",
      "Epoch 96/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6211 - val_loss: 32.3550\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 32.33806\n",
      "Epoch 97/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6224 - val_loss: 32.3480\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 32.33806\n",
      "Epoch 98/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6169 - val_loss: 32.3473\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 32.33806\n",
      "Epoch 99/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6093 - val_loss: 32.3308\n",
      "\n",
      "Epoch 00099: val_loss improved from 32.33806 to 32.33079, saving model to output/ml-20m-vae.h5\n",
      "Epoch 100/100\n",
      "61820/61820 [==============================] - 1s 15us/step - loss: 32.6113 - val_loss: 32.3535\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 32.33079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7feb668b4c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size = 100\n",
    "batch_size = 256\n",
    "mcheck = ModelCheckpoint(\n",
    "    'output/ml-20m-vae.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "es_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "model.fit(\n",
    "    np.array(X_train_vector),\n",
    "    np.array(y_train_vector),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_size,\n",
    "    validation_data=(\n",
    "        np.array(X_val_vector),\n",
    "        np.array(y_val_vector)),\n",
    "    callbacks=[\n",
    "        mcheck,\n",
    "        es_cb],\n",
    "    shuffle=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.5629579415529582\n"
     ]
    }
   ],
   "source": [
    "# テストデータに対するRMSE計算\n",
    "from sklearn.metrics import mean_squared_error\n",
    "best_model_vae = vae.build_vae()\n",
    "best_model_vae.load_weights('output/ml-20m-vae.h5')\n",
    "y_pred = best_model_vae.predict(np.array(X_test_vector))\n",
    "rmse_ = np.sqrt(mean_squared_error(y_pred, np.array(y_test_vector)))\n",
    "print('rmse: {}'.format(rmse_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(VAEのやりかたが悪かったようなだけな気もするが)今回は素のautoencoderを採用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138389/138389 [00:21<00:00, 6388.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# 評価対象のユーザ\n",
    "test_adventure_pos_items_dict = {}\n",
    "for i in tqdm(range(X_test.shape[0])):\n",
    "    # trainでadventureにアクションしていないユーザに\n",
    "    rated_items = X_train[i, :].indices\n",
    "    if len([v for v in rated_items if 'Adventure' in itemid_genres_dict[v]]) == 0:\n",
    "        # X_testの中でstoreしているアイテムが0以上のユーザに\n",
    "        if X_test[i, :].nnz > 0:\n",
    "            test_items = []\n",
    "            selected_user_ratings = X_test[i, :]\n",
    "            value_indices = selected_user_ratings.indices\n",
    "            sorted_indices = np.argsort(-X_test[i, :].toarray())[0]\n",
    "            # valueがあるアイテムのジャンルがadventureの場合に\n",
    "            for v in sorted_indices[:len(value_indices)]:\n",
    "                if 'Adventure' in itemid_genres_dict[v]:\n",
    "                    test_items.append(v)\n",
    "            if len(test_items) > 0:\n",
    "                test_adventure_pos_items_dict[i] = test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adventureのitemのベクトル\n",
    "adventure_item_vectors = adventure_ALS.item_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18319/18319 [00:28<00:00, 641.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.recommend_util import ndcg\n",
    "ndcgs = {\n",
    "    'ndcg5':  [],\n",
    "    'ndcg10':  [],\n",
    "    'ndcg20':  [],\n",
    "    'ndcg50':  [],\n",
    "    'ndcg100':  []\n",
    "}\n",
    "count = 0\n",
    "best_model = load_model('output/ml-20m-model.h5')\n",
    "\n",
    "for userid, pos_items in tqdm(test_adventure_pos_items_dict.items()):\n",
    "   # pos_itemsをadventure_matrixの次元に変換する\n",
    "    pos_items = np.array([adventure_concat_itemid_dict[v] for v in pos_items])\n",
    "    # useridに対応するユーザベクトル(action)を得る\n",
    "    try:\n",
    "        action_userid = action_train_action_users[userid]\n",
    "    except:\n",
    "        count += 1\n",
    "        # 推薦できないユーザの場合は無条件で0を入れる\n",
    "        ndcgs['ndcg5'].append(0)\n",
    "        ndcgs['ndcg10'].append(0)\n",
    "        ndcgs['ndcg20'].append(0)\n",
    "        ndcgs['ndcg50'].append(0)\n",
    "        ndcgs['ndcg100'].append(0)\n",
    "        continue\n",
    "        \n",
    "    action_user_vector = action_ALS_user_vectors[action_userid, :]\n",
    "    # autoencoderを使ってadventureの次元に変換する\n",
    "    adventure_user_vector_action_AE = best_model.predict(action_user_vector.reshape(1, -1))\n",
    "    # adventureのitemのベクトルと掛け合わせる\n",
    "    adv_predict = np.dot(adventure_user_vector_action_AE, adventure_item_vectors.T)\n",
    "    # sum_ratingsをargsort\n",
    "    sorted_indices = np.array([v for v in np.argsort(-adv_predict)])[0]\n",
    "    ndcgs['ndcg5'].append(ndcg(sorted_indices[:5], pos_items))\n",
    "    ndcgs['ndcg10'].append(ndcg(sorted_indices[:10], pos_items))\n",
    "    ndcgs['ndcg20'].append(ndcg(sorted_indices[:20], pos_items))\n",
    "    ndcgs['ndcg50'].append(ndcg(sorted_indices[:50], pos_items))\n",
    "    ndcgs['ndcg100'].append(ndcg(sorted_indices[:100], pos_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@5: 0.12771399604572783\n",
      "ndcg@10: 0.166220083945197\n",
      "ndcg@20: 0.21087733627423522\n",
      "ndcg@50: 0.280025326182347\n",
      "ndcg@100: 0.3282249752382082\n"
     ]
    }
   ],
   "source": [
    "print(\"ndcg@5: {}\".format(np.mean(ndcgs['ndcg5'])))\n",
    "print(\"ndcg@10: {}\".format(np.mean(ndcgs['ndcg10'])))\n",
    "print(\"ndcg@20: {}\".format(np.mean(ndcgs['ndcg20'])))\n",
    "print(\"ndcg@50: {}\".format(np.mean(ndcgs['ndcg50'])))\n",
    "print(\"ndcg@100: {}\".format(np.mean(ndcgs['ndcg100'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
