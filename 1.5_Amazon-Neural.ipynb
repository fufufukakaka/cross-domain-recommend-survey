{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import implicit\n",
    "\n",
    "# read data\n",
    "ratings = pd.read_feather('data/amazon_review_ratings.feather')\n",
    "\n",
    "# indexing ids\n",
    "# userid\n",
    "userid_unique = pd.Series(ratings[\"userId\"].unique())\n",
    "index_userid_dict = userid_unique.to_dict()\n",
    "# inverse\n",
    "userid_index_dict = dict(map(reversed, index_userid_dict.items()))\n",
    "\n",
    "# itemid\n",
    "itemid_unique = pd.Series(ratings[\"itemId\"].unique())\n",
    "index_itemid_dict = itemid_unique.to_dict()\n",
    "# inverse\n",
    "itemid_index_dict = dict(map(reversed, index_itemid_dict.items()))\n",
    "\n",
    "ratings[\"userId_reindex\"] = ratings[\"userId\"].map(userid_index_dict)\n",
    "ratings[\"itemid_reindex\"] = ratings[\"itemId\"].map(itemid_index_dict)\n",
    "\n",
    "# reindexしたidを使って、アイテムとジャンルの対応が取れるdictを作る\n",
    "itemid_genres_dict = ratings[['itemid_reindex', 'category']].set_index('itemid_reindex')['category'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "X_train = cloudpickle.load(open(\"output/Amazon-X_train.pkl\",\"rb\"))\n",
    "X_test = cloudpickle.load(open(\"output/Amazon-X_test.pkl\",\"rb\"))\n",
    "test_movies_and_TVs_pos_items_dict = cloudpickle.load(open('output/test_movies_and_TVs_pos_items_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregateのtrainをbookとmoviesに分離する\n",
    "# bookの列\n",
    "book_columns = [v for v in range(X_train.shape[1]) if 'book' in itemid_genres_dict[v]]\n",
    "# moviesの列\n",
    "movies_columns = [v for v in range(X_train.shape[1]) if 'movies_and_TVs' in itemid_genres_dict[v]]\n",
    "\n",
    "# 選んだカラムに応じてとってくる\n",
    "book_train = X_train[:, book_columns]\n",
    "movies_train = X_train[:, movies_columns]\n",
    "\n",
    "# moviesのみ、アイテムidのconcatとの対応関係が必要なので辞書として持っておく\n",
    "movies_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'movies_and_TVs' in itemid_genres_dict[v]:\n",
    "        movies_concat_itemid_dict[v] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アイテムidのconcatとの対応関係が必要なので辞書として持っておく\n",
    "book_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'book' in itemid_genres_dict[v]:\n",
    "        book_concat_itemid_dict[v] = count\n",
    "        count += 1\n",
    "# inverse\n",
    "inverse_book_concat_itemid_dict = dict(map(reversed, book_concat_itemid_dict.items()))\n",
    "\n",
    "movies_concat_itemid_dict = {}\n",
    "count = 0\n",
    "for v in range(X_train.shape[1]):\n",
    "    if 'movies_and_TVs' in itemid_genres_dict[v]:\n",
    "        movies_concat_itemid_dict[v] = count\n",
    "        count += 1\n",
    "# inverse\n",
    "inverse_movies_concat_itemid_dict = dict(map(reversed, movies_concat_itemid_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれにアクションしていないユーザを削る\n",
    "# 全ユーザと、削ったあとでの対応関係を辞書として持っておく\n",
    "book_train_selected = book_train[book_train.getnnz(1)>0]\n",
    "movies_train_selected = movies_train[movies_train.getnnz(1)>0]\n",
    "\n",
    "book_train_action_users = {}\n",
    "book_users = book_train.getnnz(1)>0\n",
    "count = 0\n",
    "for i in range(book_train.shape[0]):\n",
    "    if book_users[i]:\n",
    "        book_train_action_users[i] = count\n",
    "        count += 1\n",
    "\n",
    "# inverse\n",
    "inverse_book_train_action_users = dict(map(reversed, book_train_action_users.items()))\n",
    "\n",
    "movies_train_action_users = {}\n",
    "movies_users = movies_train.getnnz(1)>0\n",
    "count = 0\n",
    "for i in range(movies_train.shape[0]):\n",
    "    if movies_users[i]:\n",
    "        movies_train_action_users[i] = count\n",
    "        count += 1\n",
    "\n",
    "# inverse\n",
    "inverse_movies_train_action_users = dict(map(reversed, movies_train_action_users.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれでALSする\n",
    "# 今回は mediateでやったときのものを使う\n",
    "book_ALS = cloudpickle.load(open('output/book_ALS.pkl', 'rb'))\n",
    "movies_ALS = cloudpickle.load(open(\"output/movies_ALS.pkl\",\"rb\"))\n",
    "\n",
    "book_ALS_user_vectors = book_ALS.user_factors\n",
    "movies_ALS_user_vectors = movies_ALS.user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690240/690240 [00:00<00:00, 917207.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# bookとmoviesでoverlapしているユーザで、ベクトルの対応表を作る\n",
    "overlap_book_user_vectors = []\n",
    "overlap_movies_user_vectors = []\n",
    "count = 0\n",
    "for u in tqdm(range(X_train.shape[0])):\n",
    "    if u in book_train_action_users and u in movies_train_action_users:\n",
    "        overlap_book_user_vectors.append(book_ALS_user_vectors[book_train_action_users[u]].tolist())\n",
    "        overlap_movies_user_vectors.append(movies_ALS_user_vectors[movies_train_action_users[u]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoderの学習をする(movielensで良かったモデルがAutoEncoderだったので、こちらではこれに絞る)\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def build_model(input_dim, output_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation='relu')(inputs)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(output_dim, activation='sigmoid')(decoded)\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae','mse'])\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11525 samples, validate on 3842 samples\n",
      "Epoch 1/100\n",
      "11525/11525 [==============================] - 1s 47us/step - loss: 0.1620 - mae: 0.3532 - mse: 0.1620 - val_loss: 0.0126 - val_mae: 0.0466 - val_mse: 0.0126\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01262, saving model to output/amazon-model.h5\n",
      "Epoch 2/100\n",
      "11525/11525 [==============================] - 0s 18us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01262 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 3/100\n",
      "11525/11525 [==============================] - 0s 17us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01251\n",
      "Epoch 4/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01251\n",
      "Epoch 5/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01251\n",
      "Epoch 6/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 7/100\n",
      "11525/11525 [==============================] - 0s 20us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 8/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 9/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 10/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 11/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 12/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 13/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 14/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 15/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 16/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 17/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 18/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 19/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 20/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 21/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 22/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 23/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 24/100\n",
      "11525/11525 [==============================] - 0s 15us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 25/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0444 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 26/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 27/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 28/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 29/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 30/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 31/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 32/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 33/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 34/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 35/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 36/100\n",
      "11525/11525 [==============================] - 0s 15us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 37/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 38/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 39/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 40/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 41/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 42/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 43/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01251 to 0.01251, saving model to output/amazon-model.h5\n",
      "Epoch 44/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0442 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.01251 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 45/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 46/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 47/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 48/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 49/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 50/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 51/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 52/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 53/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 54/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0445 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 55/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 56/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 57/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 58/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 59/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 60/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 61/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 62/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 63/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 64/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 65/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 66/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 67/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 68/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 69/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 70/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 71/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 72/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 73/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 74/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.01250 to 0.01250, saving model to output/amazon-model.h5\n",
      "Epoch 75/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.01250 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 76/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.01249 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 77/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.01249 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 78/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.01249 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 79/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.01249 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 80/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.01249 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 81/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.01249 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 82/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.01249 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 83/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.01249 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 84/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0447 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01249\n",
      "Epoch 85/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.01249 to 0.01249, saving model to output/amazon-model.h5\n",
      "Epoch 86/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.01249 to 0.01248, saving model to output/amazon-model.h5\n",
      "Epoch 87/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.01248 to 0.01248, saving model to output/amazon-model.h5\n",
      "Epoch 88/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.01248 to 0.01248, saving model to output/amazon-model.h5\n",
      "Epoch 89/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.01248 to 0.01248, saving model to output/amazon-model.h5\n",
      "Epoch 90/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01248\n",
      "Epoch 91/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.01248 to 0.01248, saving model to output/amazon-model.h5\n",
      "Epoch 92/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01248\n",
      "Epoch 93/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01248\n",
      "Epoch 94/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01248\n",
      "Epoch 95/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0122 - mae: 0.0446 - mse: 0.0122 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01248\n",
      "Epoch 96/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0121 - mae: 0.0446 - mse: 0.0121 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01248\n",
      "Epoch 97/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0121 - mae: 0.0446 - mse: 0.0121 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01248\n",
      "Epoch 98/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0121 - mae: 0.0445 - mse: 0.0121 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.01248 to 0.01248, saving model to output/amazon-model.h5\n",
      "Epoch 99/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0121 - mae: 0.0446 - mse: 0.0121 - val_loss: 0.0125 - val_mae: 0.0444 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01248\n",
      "Epoch 100/100\n",
      "11525/11525 [==============================] - 0s 16us/step - loss: 0.0121 - mae: 0.0445 - mse: 0.0121 - val_loss: 0.0125 - val_mae: 0.0443 - val_mse: 0.0125\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.01248 to 0.01248, saving model to output/amazon-model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc4ddaa8860>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_vector, X_test_vector, y_train_vector, y_test_vector = train_test_split(overlap_book_user_vectors, overlap_movies_user_vectors, random_state=42)\n",
    "X_train_vector, X_val_vector, y_train_vector, y_val_vector = train_test_split(X_train_vector, y_train_vector, random_state=42)\n",
    "\n",
    "epoch_size = 100\n",
    "batch_size = 256\n",
    "mcheck = ModelCheckpoint(\n",
    "    'output/amazon-model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "es_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto')\n",
    "model = build_model(np.array(X_train_vector).shape[1], np.array(y_train_vector).shape[1])\n",
    "model.fit(\n",
    "    np.array(X_train_vector),\n",
    "    np.array(y_train_vector),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_size,\n",
    "    validation_data=(\n",
    "        np.array(X_val_vector),\n",
    "        np.array(y_val_vector)),\n",
    "    callbacks=[\n",
    "        mcheck,\n",
    "        es_cb],\n",
    "    shuffle=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.11936578717966138\n"
     ]
    }
   ],
   "source": [
    "# テストデータに対するRMSE計算\n",
    "from sklearn.metrics import mean_squared_error\n",
    "best_model = load_model('output/amazon-model.h5')\n",
    "y_pred = best_model.predict(np.array(X_test_vector))\n",
    "rmse_ = np.sqrt(mean_squared_error(y_pred, np.array(y_test_vector)))\n",
    "print('rmse: {}'.format(rmse_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moviesのitemのベクトル\n",
    "movies_item_vectors = movies_ALS.item_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10250/10250 [01:02<00:00, 163.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from lib.recommend_util import ndcg\n",
    "ndcgs = {\n",
    "    'ndcg5':  [],\n",
    "    'ndcg10':  [],\n",
    "    'ndcg20':  [],\n",
    "    'ndcg50':  [],\n",
    "    'ndcg100':  []\n",
    "}\n",
    "count = 0\n",
    "for userid, pos_items in tqdm(test_movies_and_TVs_pos_items_dict.items()):\n",
    "   # pos_itemsをmovies_matrixの次元に変換する\n",
    "    pos_items = np.array([movies_concat_itemid_dict[v] for v in pos_items])\n",
    "    # useridに対応するユーザベクトル(book)を得る\n",
    "    try:\n",
    "        book_userid = book_train_action_users[userid]\n",
    "        book_user_vector = book_ALS_user_vectors[book_userid, :]\n",
    "        # AutoEncoderを使ってmoviesの次元に変換する\n",
    "        movies_user_vector_action_AE = best_model.predict(book_user_vector.reshape(1, -1))\n",
    "        # adventureのitemのベクトルと掛け合わせる\n",
    "        movies_predict = np.dot(movies_user_vector_action_AE, movies_item_vectors.T)\n",
    "        # sum_ratingsをargsort\n",
    "        sorted_indices = np.array([v for v in np.argsort(-movies_predict)])[0]\n",
    "        ndcgs['ndcg5'].append(ndcg(sorted_indices[:5], pos_items))\n",
    "        ndcgs['ndcg10'].append(ndcg(sorted_indices[:10], pos_items))\n",
    "        ndcgs['ndcg20'].append(ndcg(sorted_indices[:20], pos_items))\n",
    "        ndcgs['ndcg50'].append(ndcg(sorted_indices[:50], pos_items))\n",
    "        ndcgs['ndcg100'].append(ndcg(sorted_indices[:100], pos_items))\n",
    "    except:\n",
    "        count += 1\n",
    "        # 推薦できないユーザの場合は無条件で0を入れる\n",
    "        ndcgs['ndcg5'].append(0)\n",
    "        ndcgs['ndcg10'].append(0)\n",
    "        ndcgs['ndcg20'].append(0)\n",
    "        ndcgs['ndcg50'].append(0)\n",
    "        ndcgs['ndcg100'].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@5: 0.0040366900247044675\n",
      "ndcg@10: 0.006191071032932619\n",
      "ndcg@20: 0.008574333528193033\n",
      "ndcg@50: 0.013658642792483655\n",
      "ndcg@100: 0.018854547775509517\n"
     ]
    }
   ],
   "source": [
    "print(\"ndcg@5: {}\".format(np.mean(ndcgs['ndcg5'])))\n",
    "print(\"ndcg@10: {}\".format(np.mean(ndcgs['ndcg10'])))\n",
    "print(\"ndcg@20: {}\".format(np.mean(ndcgs['ndcg20'])))\n",
    "print(\"ndcg@50: {}\".format(np.mean(ndcgs['ndcg50'])))\n",
    "print(\"ndcg@100: {}\".format(np.mean(ndcgs['ndcg100'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
